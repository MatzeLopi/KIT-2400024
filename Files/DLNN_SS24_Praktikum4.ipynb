{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsaT3ibpVIOb"
      },
      "source": [
        "# Praktikum 4 - Knowledge Distillation\n",
        "\n",
        "Note: the praktikums are for your own practice. They will **not be graded**!\n",
        "\n",
        "Remember to make a copy of this notebook to your own Colab. Changes made directly here will not be stored!\n",
        "\n",
        "Whenenver you see an ellipsis `...` and/or TODO comment, you're supposed to insert code or text answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TplPFY5sVIOb"
      },
      "source": [
        "In this notebook, we will learn about knowledge distillation.\n",
        "\n",
        "We will run a number of experiments focused at\n",
        "improving the accuracy of a lightweight neural network, using a more\n",
        "powerful network as a teacher. In particular, we will look at:\n",
        "\n",
        "-   How to modify model classes to extract hidden representations and\n",
        "    use them for further calculations\n",
        "-   How to modify regular train loops in PyTorch to include additional\n",
        "    losses on top of, for example, cross-entropy for classification\n",
        "-   How to improve the performance of lightweight models by using more\n",
        "    complex models as teachers\n",
        "\n",
        "Prerequisites\n",
        "=============\n",
        "\n",
        "-   1 GPU, 4GB of memory\n",
        "-   PyTorch v2.0 or later\n",
        "-   CIFAR-10 dataset (downloaded by the script and saved in a directory\n",
        "    called `/data`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StBRxwkXVIOb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Check if GPU is available, and if not, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tURFSoCVIOb"
      },
      "source": [
        "Loading CIFAR-10\n",
        "================\n",
        "\n",
        "CIFAR-10 is a popular image dataset with ten classes. Our objective is\n",
        "to predict one of the following classes for each input image.\n",
        "\n",
        "![Example of CIFAR-10\n",
        "images](https://pytorch.org/tutorials//../_static/img/cifar10.png)\n",
        "\n",
        "The input images are RGB, and are 32x32 pixels. Each image is described by 3 x 32 x 32 = 3072 numbers ranging from 0 to 255. We normalize the input by subtracting the mean and dividing by the standard deviation along each channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXy_w5aXVIOc"
      },
      "outputs": [],
      "source": [
        "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
        "transforms_cifar = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # mean and std are pre-computed\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 dataset:\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv5m5Kz6VIOd"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>This section is for CPU users only who are interested in quick results. Use this option only if you're interested in a small scale experiment. Select only the first <code>num_images_to_keep</code> images from the train/test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "num_images_to_keep = 2000\n",
        "train_dataset = Subset(train_dataset, range(min(num_images_to_keep, 50_000)))\n",
        "test_dataset = Subset(test_dataset, range(min(num_images_to_keep, 10_000)))\n"
      ],
      "metadata": {
        "id": "Wtnxb8S_kEFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLzQdld-VIOd"
      },
      "outputs": [],
      "source": [
        "#Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8fFdRhlVIOd"
      },
      "source": [
        "Defining model classes and utility functions\n",
        "============================================\n",
        "\n",
        "Next, we need to define our model classes. We use two different architectures,\n",
        "keeping the number of filters fixed across our experiments to ensure\n",
        "fair comparisons. Both architectures are CNNs with a different number of convolutional layers, followed by a classifier with 10 classes.\n",
        "\n",
        "The number of layers and neurons are smaller for the students.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6065d3LpVIOd"
      },
      "outputs": [],
      "source": [
        "# Deeper neural network class to be used as teacher:\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Lightweight neural network class to be used as student:\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp8lDd_GVIOe"
      },
      "source": [
        "We employ 2 functions to help us produce and evaluate the results on our\n",
        "original classification task, one for training and one for testing.\n",
        "\n",
        "![Train both networks with Cross-Entropy. The student will be used as a\n",
        "baseline:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/ce_only.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLwyC962VIOe"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs: A collection of batch_size images\n",
        "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
        "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HguAMDV8VIOe"
      },
      "source": [
        "Cross-entropy runs\n",
        "==================\n",
        "\n",
        "For reproducibility, we need to set the torch manual seed.\n",
        "\n",
        "Start by training the teacher network using cross-entropy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlWuWH5cVIOe"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)  # for reproducibility\n",
        "\n",
        "nn_deep = DeepNN(num_classes=10).to(device)\n",
        "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
        "\n",
        "# Instantiate the lightweight network:\n",
        "torch.manual_seed(42)\n",
        "nn_light = LightNN(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BezAnW_VIOe"
      },
      "source": [
        "We instantiate one more lightweight network model to compare their\n",
        "performances. Back propagation is sensitive to weight initialization, so\n",
        "we need to make sure these two networks have the exact same\n",
        "initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzbSqc6NVIOe"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "new_nn_light = LightNN(num_classes=10).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF5YFkN4VIOe"
      },
      "source": [
        "To ensure we have created a copy of the first network, we inspect the\n",
        "norm of its first layer. If it matches, then we are safe to conclude\n",
        "that the networks are indeed the same.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiaNmzTjVIOe"
      },
      "outputs": [],
      "source": [
        "# Print the norm of the first layer of the initial lightweight model\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
        "# Print the norm of the first layer of the new lightweight model\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k45LvXUYVIOe"
      },
      "source": [
        "Print the total number of parameters in each model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nB7-ybGVIOe"
      },
      "outputs": [],
      "source": [
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6Hs6ZIMVIOe"
      },
      "source": [
        "Train and test the lightweight network with cross entropy loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKRygfe5VIOe"
      },
      "outputs": [],
      "source": [
        "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "test_accuracy_light_ce = test(nn_light, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_aEYHibVIOe"
      },
      "source": [
        "As we can see, based on test accuracy, we can now compare the deeper\n",
        "network that is to be used as a teacher with the lightweight network\n",
        "that is our supposed student. So far, our student has not intervened\n",
        "with the teacher, therefore this performance is achieved by the student\n",
        "itself. The metrics so far can be seen with the following lines:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSioDze4VIOe"
      },
      "outputs": [],
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhOOdljBVIOe"
      },
      "source": [
        "Knowledge distillation run\n",
        "==========================\n",
        "\n",
        "Now let\\'s try to improve the test accuracy of the student network by\n",
        "incorporating the teacher.\n",
        "\n",
        "We will incorporating an additional loss (i.e., the distillation loss) into the traditional cross entropy loss.\n",
        "\n",
        "The distillation loss measures how the teacher's output probability distribution is different to the student's.\n",
        "\n",
        "However, in many cases, the teacher's probability distribution has the correct class at a very high probability, with all other class probabilities very close to 0. As such, it doesn't provide much information beyond the ground truth labels already provided in the dataset. To tackle this issue, we soften output distributions by scaling the logits with the temperature $T$:\n",
        "\n",
        "$$\n",
        "\\frac{z_i}{T}\n",
        "$$\n",
        "\n",
        "\n",
        "Larger `T` leads to smoother distributions, thus smaller probabilities get a larger boost.\n",
        "\n",
        "\n",
        "The distillation loss is then calculated using Kullback-Leibler divergence:\n",
        "\n",
        "$$\n",
        "L_{distil} = \\frac{âˆ‘_{x \\in X} P(x) log(\\frac{P(x)}{Q(x)})}{|X|} \\times T^2\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$P$ is the output distribution of the teacher, after applying softmax onto the logits.\n",
        "\n",
        "$Q$ is the output distribution of the student, after applying softmax onto the logits.\n",
        "\n",
        "$X$ is the sample space.\n",
        "\n",
        "\n",
        "\n",
        "We also defines 2 hyperparameters to balance between the two losses:\n",
        "-   `soft_target_loss_weight`: A weight assigned to the extra objective\n",
        "    we\\'re about to include.\n",
        "-   `ce_loss_weight`: A weight assigned to cross-entropy. Tuning these\n",
        "    weights pushes the network towards optimizing for either objective.\n",
        "\n",
        "![Distillation loss is calculated from the logits of the networks. It\n",
        "only returns gradients to the\n",
        "student:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/distillation_output_loss.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oNtehR-VIOe"
      },
      "outputs": [],
      "source": [
        "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            teacher_logits = ...\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = ...\n",
        "\n",
        "            # Calculate the softened output distributions\n",
        "            soft_lprob_teacher = ...\n",
        "            soft_lprob_student = ...\n",
        "\n",
        "            # Calculate the distilation loss\n",
        "            distilation_loss = ...\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ...\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = ...\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
        "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
        "\n",
        "# Compare the student test accuracy with and without the teacher, after distillation\n",
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to play around with the temperature parameter that controls\n",
        "the softness of the softmax function and the loss coefficients."
      ],
      "metadata": {
        "id": "foH_PSQTdO8e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hTlQfABVIOe"
      },
      "source": [
        "Cosine loss minimization run\n",
        "============================\n",
        "\n",
        "In this section, instead of minimizing the teacher-student difference in the final probability distributions, we try to minimize the difference in the hidden states.\n",
        "\n",
        "We wil include a naive loss function that helps increasing the cosine similarity of the flattened vectors.\n",
        "\n",
        "\n",
        "We will be using the\n",
        "`CosineEmbeddingLoss` which is given by the following formula:\n",
        "\n",
        "![Formula for\n",
        "CosineEmbeddingLoss](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/cosine_embedding_loss.png)\n",
        "\n",
        "\n",
        "However, recall that the hidden dimension size of the teacher model is larger that that of the student model. Therefore, we will includ an average pooling\n",
        "layer after the teacher\\'s convolutional layer to reduce its\n",
        "dimensionality to match that of the student.\n",
        "\n",
        "To proceed, we will modify our model classes, or create new ones. Now,\n",
        "the forward function returns not only the logits of the network but also\n",
        "the flattened hidden representation after the convolutional layer. We\n",
        "include the aforementioned pooling for the modified teacher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ0LOQ9dVIOe"
      },
      "outputs": [],
      "source": [
        "class ModifiedDeepNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        flattened_conv_output = ...\n",
        "\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "\n",
        "        flattened_conv_output_after_pooling = ...\n",
        "\n",
        "        return x, flattened_conv_output_after_pooling\n",
        "\n",
        "# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\n",
        "class ModifiedLightNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        flattened_conv_output = ...\n",
        "\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        return x, flattened_conv_output\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
        "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Once again ensure the norm of the first layer is the same for both networks\n",
        "print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n",
        "print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
        "\n",
        "# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
        "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMrOvK8-VIOf"
      },
      "source": [
        "Naturally, we need to change the train loop because now the model\n",
        "returns a tuple `(logits, hidden_representation)`. Using a sample input\n",
        "tensor we can print their shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW9iViIxVIOf"
      },
      "outputs": [],
      "source": [
        "# Create a sample input tensor\n",
        "sample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32\n",
        "\n",
        "# Pass the input through the student\n",
        "logits, hidden_representation = modified_nn_light(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
        "\n",
        "# Pass the input through the teacher\n",
        "logits, hidden_representation = modified_nn_deep(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpi7X3vwVIOf"
      },
      "source": [
        "In our case, `hidden_representation_size` is `1024`. This is the\n",
        "flattened feature map of the final convolutional layer of the student\n",
        "and the input for its classifier. It is `1024` for\n",
        "the teacher too, because we made it so with `avg_pool1d` from `2048`.\n",
        "The loss applied here only affects the weights of the student prior to\n",
        "the loss calculation. In other words, it does not affect the classifier\n",
        "of the student. The modified training loop is the following:\n",
        "\n",
        "![In Cosine Loss minimization, we want to maximize the cosine similarity\n",
        "of the two representations by returning gradients to the\n",
        "student:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/cosine_loss_distillation.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIbkT_AZVIOf"
      },
      "outputs": [],
      "source": [
        "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    cosine_loss = nn.CosineEmbeddingLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model and keep only the hidden representation\n",
        "            _, teacher_hidden_representation = ...\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, student_hidden_representation = ...\n",
        "\n",
        "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
        "            hidden_rep_loss = ...\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ...\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = ...\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsUAzPV6VIOf"
      },
      "source": [
        "We need to modify our test function for the same reason. Here we ignore\n",
        "the hidden representation returned by the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtMxsT7oVIOf"
      },
      "outputs": [],
      "source": [
        "def test_multiple_outputs(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baaPWRp4VIOf"
      },
      "source": [
        "We could also easily include both knowledge distillation and\n",
        "cosine loss minimization in the same function. It is common to combine\n",
        "methods to achieve better performance in teacher-student paradigms.\n",
        "\n",
        "For now, we just run a simple train-test session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxLzUQfqVIOf"
      },
      "outputs": [],
      "source": [
        "# Train and test the lightweight network with cross entropy loss\n",
        "train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxKwqrbzVIOf"
      },
      "source": [
        "Intermediate regressor run\n",
        "==========================\n",
        "\n",
        "Our naive minimization does not guarantee better results for several\n",
        "reasons, one being the dimensionality of the vectors. Cosine similarity\n",
        "generally works better than Euclidean distance for vectors of higher\n",
        "dimensionality, but we were dealing with vectors with 1024 components\n",
        "each, so it is much harder to extract meaningful similarities.\n",
        "\n",
        "Furthermore, pushing towards a match of the hidden\n",
        "representation of the teacher and the student is not supported by\n",
        "theory, since the two models could have learnt things in different ways.\n",
        "\n",
        "We will provide a final example of training\n",
        "intervention by including an extra network called regressor. The\n",
        "objective is to first extract the feature map of the teacher after a\n",
        "convolutional layer, then extract a feature map of the student after a\n",
        "convolutional layer, and finally try to match these maps.\n",
        "\n",
        "This time, we will introduce a regressor between the networks to facilitate\n",
        "the matching process. The regressor will be trainable and ideally will\n",
        "do a better job than our naive cosine loss minimization scheme. Its main\n",
        "job is to match the dimensionality of these feature maps so that we can\n",
        "properly define a loss function between the teacher and the student.\n",
        "\n",
        "\n",
        "Defining such a loss function provides a teaching \\\"path\\\", which is\n",
        "basically a flow to back-propagate gradients that will change the\n",
        "student\\'s weights. Focusing on the output of the convolutional layers\n",
        "right before each classifier for our original networks, we have the\n",
        "following shapes:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epi4cIlFVIOf"
      },
      "outputs": [],
      "source": [
        "# Pass the sample input only from the convolutional feature extractor\n",
        "convolutional_fe_output_student = nn_light.features(sample_input)\n",
        "convolutional_fe_output_teacher = nn_deep.features(sample_input)\n",
        "\n",
        "# Print their shapes\n",
        "print(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)\n",
        "print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9zPFmk5VIOf"
      },
      "source": [
        "We have 32 filters for the teacher and 16 filters for the student. We\n",
        "will include a trainable layer that converts the feature map of the\n",
        "student to the shape of the feature map of the teacher. In practice, we\n",
        "modify the lightweight class to return the hidden state after an\n",
        "intermediate regressor that matches the sizes of the convolutional\n",
        "feature maps and the teacher class to return the output of the final\n",
        "convolutional layer without pooling or flattening.\n",
        "\n",
        "![The trainable layer matches the shapes of the intermediate tensors and\n",
        "Mean Squared Error (MSE) is properly\n",
        "defined:](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/fitnets_knowledge_distill.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo2bzZ22VIOi"
      },
      "outputs": [],
      "source": [
        "class ModifiedDeepNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedDeepNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "\n",
        "        conv_feature_map = ...\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x, conv_feature_map\n",
        "\n",
        "class ModifiedLightNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedLightNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Include an extra regressor\n",
        "        self.regressor = ...\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        regressor_output = ...\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x, regressor_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsV0WwieVIOi"
      },
      "source": [
        "After that, we have to update our train loop again. This time, we\n",
        "extract the regressor output of the student, the feature map of the\n",
        "teacher, we calculate the `MSE` on these tensors (they have the exact\n",
        "same shape so it\\'s properly defined) and we back propagate gradients\n",
        "based on that loss, in addition to the regular cross entropy loss of the\n",
        "classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prs4y6zCVIOi"
      },
      "outputs": [],
      "source": [
        "def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    mse_loss = nn.MSELoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train() # Student to train mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Again ignore teacher logits\n",
        "            _, teacher_feature_map = ...\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, regressor_feature_map = ...\n",
        "\n",
        "            # Calculate the loss\n",
        "            hidden_rep_loss = ...\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ...\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = ...\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.\n",
        "\n",
        "# Initialize a ModifiedLightNNRegressor\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)\n",
        "modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Train and test once again\n",
        "train_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, device=device)\n",
        "test_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvbLaL1uVIOi"
      },
      "source": [
        "It is expected that the final method will work better than `CosineLoss`\n",
        "because now we have allowed a trainable layer between the teacher and\n",
        "the student, which gives the student some wiggle room when it comes to\n",
        "learning, rather than pushing the student to copy the teacher\\'s\n",
        "representation. Including the extra network is the idea behind\n",
        "hint-based distillation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2wutAPnVIOi"
      },
      "outputs": [],
      "source": [
        "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
        "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
        "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
        "print(f\"Student accuracy with CE + CosineLoss: {test_accuracy_light_ce_and_cosine_loss:.2f}%\")\n",
        "print(f\"Student accuracy with CE + RegressorMSE: {test_accuracy_light_ce_and_mse_loss:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geGa1HQNVIOi"
      },
      "source": [
        "Conclusion\n",
        "==========\n",
        "\n",
        "None of the methods above increases the number of parameters for the\n",
        "network or inference time, so the performance increase comes at the\n",
        "little cost of calculating gradients during training. In ML\n",
        "applications, we mostly care about inference time because training\n",
        "happens before the model deployment."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGxB2FJjBesX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}