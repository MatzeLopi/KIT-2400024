{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatzeLopi/KIT-2400024/blob/main/DLNN_SS24_Praktikum1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b471288db3b44cfb992b6c07f7c3fd83",
        "deepnote_cell_type": "text-cell-h1",
        "formattedRanges": [],
        "id": "w6spdQYQ2tgd",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "# Praktikum 1 - Simple Neural Network with Numpy and Pytorch\n",
        "\n",
        "Note: the praktikums are for your own practice. They will **not be graded**!\n",
        "\n",
        "You have around one week to work on it. Then we will go over the solutions together in the praktikum time slots!\n",
        "\n",
        "Remember to make a copy of this notebook to your own Colab. Changes made directly here will not be stored!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7b2ab899b012471f80f00f1ff3b7992a",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "xWJ_eyKS2tge",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "In this praktikum, we will build a perceptron model and a Multi-Layer Perceptron (MLP) model **from scratch** with numpy. This is intended to help you understand in details the internal working of neural networks.\n",
        "\n",
        "Next, we will proceed with building an MLP with [**Pytorch**](https://pytorch.org/get-started/locally/), which is a widely-used framework for building deep learning models.\n",
        "\n",
        "Our first challenge is solving the **XOR task** that you've seen in the lecture, before we move to a slightly more complex problem, namely the **Iris dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dfb46b3d43444b14961ecd9967845ea1",
        "deepnote_cell_height": 141.953125,
        "deepnote_cell_type": "markdown",
        "id": "pOIoDJv_2tgf",
        "tags": []
      },
      "source": [
        "**Notice**: Whenenver you see an ellipsis `...` or TODO comment, you're supposed to insert code or text answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "10a3e57eb4844285b70b25bcd41c834e",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "M8QD3-q02tgg",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Excercise 1: XOR Task with Single Perceptron (from scratch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "84d575c151ba447192c5bc8d6bf696e1",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "B4XIvrKj2tgh",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "XOR (exclusive OR) is a logic function that gives 1 as an output when the number of true inputs is odd, otherwise it outputs a 0. Our goal is to model this function using neurons. We'll start with a single neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c9900820a8204251a55585f98126c98e",
        "deepnote_cell_height": 276.84375,
        "deepnote_cell_type": "markdown",
        "id": "ekh7hOIk2tgh",
        "tags": []
      },
      "source": [
        "<center><img src=\"https://www.xplore-dna.net/pluginfile.php/286/mod_page/content/21/Tabelle%20-%20XOR.png\" width=\"250\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "eeef0f122a2e45be9ab17634e036b17f",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "RS83rXR82tgi",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "Let's start with importing some necessary dependencies that we will need throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "95e9b1a1f9494ea89f65aee06be516dc",
        "deepnote_cell_height": 61,
        "deepnote_cell_type": "code",
        "id": "-nrjhMBO2tgj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "39505416089744699256a5844dd99d19",
        "deepnote_cell_height": 252.9375,
        "deepnote_cell_type": "markdown",
        "id": "dlGxHaM72tgk",
        "tags": []
      },
      "source": [
        "In the first part of this exercise you'll build a perceptron, a single neuron, that takes two binary input values and returns a binary output value.\n",
        "\n",
        "<center><img src=\"https://i.stack.imgur.com/eBSki.jpg\" width=\"280\" />\n",
        "\n",
        "<center><img src=\"\" width=\"280\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "816aceffd3b045029c7d82becc35489e",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "I7PmyXz92tgk",
        "tags": []
      },
      "source": [
        "Perceptron can be seen as a single neuron, mapping an input $\\textbf{x}$ to an output $o$ using weights $\\textbf{w}$ and a bias $b$. $\\cdot$ is the dot product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "11a6fcabddb143599d0b7bc55f5cc50c",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "7UjfCaeq2tgl",
        "tags": []
      },
      "source": [
        "$o = \\textbf{w}\\cdot \\textbf{x}+b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "59d21d8a14cf4ffab7daf30a94c91be8",
        "deepnote_cell_height": 110.78125,
        "deepnote_cell_type": "markdown",
        "id": "QW5c6tT52tgl",
        "tags": []
      },
      "source": [
        "#### Perceptron Update Rule\n",
        "\n",
        "\n",
        "Perceptron Update Rule is a process that is specific to the training of a single-perceptron model, which we can apply to binary classification problems. This process [has been proven to converge](https://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf) if the data is linearly seperatable and the learning rate is small enough.\n",
        "\n",
        "\n",
        "Let's use it here to have a first baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fe3ac47eb7134fbd9e92b9b887dcda12",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "5dNqpPdd2tgl",
        "tags": []
      },
      "source": [
        "For classification problems $0>o$ is interpreted as class 1, and $o<0$ is interpreted as class 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6454ee77e7e4411f8d341729e17e90a5",
        "deepnote_cell_height": 262.734375,
        "deepnote_cell_type": "markdown",
        "id": "CWqEU0Ds2tgl",
        "tags": []
      },
      "source": [
        "For updating the associated weights, we can use the following update rule:\n",
        "\n",
        "$w_i = w_i + \\nabla w_i$\n",
        "\n",
        "where\n",
        "\n",
        "$\\nabla w_i = \\eta(t-o)x_i$\n",
        "\n",
        "- $t$ is the target\n",
        "- $o$ is the output\n",
        "- $\\eta$ is the learning rate (a small constant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "89280b788ec34866ab8f8ead1cc686c3",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "oCy6z6Ig2tgm",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Implementation of a Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "82eb15fa556f4017824bfc25bdfa9b71",
        "deepnote_cell_height": 871,
        "deepnote_cell_type": "code",
        "id": "eNs7zHyT2tgm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class perceptron_implementation():\n",
        "    def __init__(self):\n",
        "        # TODO:\n",
        "        # Initialize weights\n",
        "        # For perceptrons, it's possible to initialize all weights with 0\n",
        "\n",
        "        self.neuron_weights = np.zeros(2)\n",
        "        self.bias = 0\n",
        "\n",
        "    def forward_pass(self, x:np.array):\n",
        "        # Implement  o = x * w + b\n",
        "        output = sum(self.neuron_weights * x) + self.bias\n",
        "\n",
        "        return output\n",
        "\n",
        "    def perceptron_update_rule(self, target:np.array, prediction:np.array, x:np.array, learning_rate:int=1):\n",
        "        # Perform perceptron update rule that is defined above\n",
        "        # use self.neuron_weights\n",
        "        # TODO\n",
        "        \n",
        "        weight_delta = learning_rate * (target - prediction) * x\n",
        "\n",
        "        new_weights = ...\n",
        "\n",
        "        self.neuron_weights = new_weights\n",
        "\n",
        "    def train(self, input_data, targets):\n",
        "        \"\"\"\n",
        "        input_data: Multi-dimensional array that contains all inputs\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        for x, y in zip(input_data, targets):\n",
        "          ...\n",
        "        # END TODO\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        # TODO\n",
        "        outputs = []\n",
        "        for x in input_data:\n",
        "            ...\n",
        "\n",
        "        return np.array(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "75e4622dce1a4d39b1734879909d04d2",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "5kpPuTNY2tgn",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "fdac76e29da047579c02f7c61cec4ffc",
        "deepnote_cell_height": 223,
        "deepnote_cell_type": "code",
        "id": "k7UXXkGL2tgn",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perceptron = perceptron_implementation()\n",
        "\n",
        "# TODO\n",
        "input_data = ...\n",
        "targets = ...\n",
        "\n",
        "# train the corresponding single neuron\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c22027433597480481df2737709639dd",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "TD9gtzyL2tgn",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_id": "c454d52152ce428c81d51ad9f34fd970",
        "deepnote_cell_height": 151,
        "deepnote_cell_type": "code",
        "id": "agv_oJ382tgn",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ellipsis\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "\n",
        "predictions = ...\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "85b73e5dbdd14488b836e0dad804b0a3",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "DcMjb5zH2tgo",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e76ebc19dbad44988015c11187bd7969",
        "deepnote_cell_height": 243.34375,
        "deepnote_cell_type": "markdown",
        "id": "ETEW4Rhp2tgo",
        "tags": []
      },
      "source": [
        "For evaluation, we will need to consider appropriate metrics. For classification tasks, **accuracy** is one of the most common metrics.\n",
        "\n",
        "It is defined as:\n",
        "\n",
        "$\\textrm{Accuracy}=\\frac{1}{N}\\sum_i^N1(y_i=\\hat{y}_i)$\n",
        "\n",
        "where $y$ is an array of our target values, and $\\hat{y}$ is an array of our predictions.\n",
        "\n",
        "For accuracy, if outputs are probabilities, there needs to be a threshold for transforming logit predictions to binary `(0,1)` predictions. We will set this threshold to `0.5`. For our perceptron this is not needed, since we already output binary values, however, we will use the `accuracy` function later on, so the predictions should be considered to be probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kpPwA9EeGm_Q"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions: np.ndarray, targets: np.ndarray, threshold=0.5) -> float:\n",
        "    # TODO\n",
        "    # Implement the accuracy metric\n",
        "    ...\n",
        "    # END TODO\n",
        "    return accuracy_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "65abd9e35d404aa5af91b1ab7d74147b",
        "deepnote_cell_height": 259,
        "deepnote_cell_type": "code",
        "id": "xJCPWPYj2tgo",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ellipsis\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# Call accuracy function and provide necessary inputs to calculate accuracy\n",
        "accuracy_value = ...\n",
        "print(accuracy_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSQ8rFEB5Q7"
      },
      "source": [
        "You will see that it is not possible to get to 100% accuracy, since XOR is not a linear-separatable problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ed9c69e1f9d7460b89f6899e938a68bf",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "9zwDNf042tgs",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Excercise 2: XOR Task with MLP (from scratch)\n",
        "\n",
        "As mentioned in the lecture, unlike a single perceptron, Multi-Layer Perceptron (MLP) can deal with problems that are non-linearly-separatable like XOR.\n",
        "\n",
        "Now we will try to implement an MLP with 3 hidden layers and a hidden dimension of 3. We will also add an activiation function to introduce nonlinearity in our hidden layers.\n",
        "\n",
        "<img src=\"https://i.imgur.com/IUQ05Ol.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "df9ec443c0284dc4a52c55b8caa88358",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "UQdPsvFa2tgq",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Initializing Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c251b1998be74c5fbc41cd1d47edaa88",
        "deepnote_cell_height": 101.46875,
        "deepnote_cell_type": "markdown",
        "id": "lV-m23SR2tgq",
        "tags": []
      },
      "source": [
        "Xavier intitialization is commonly used to initialize the weights of a network. It is a random uniform distribution that’s bounded between $\\pm\\frac{\\sqrt{6}}{\\sqrt{n_i+n_{i+1}}}$ where $n_i$ is the number of incoming network connections, and $n_{i+1}$ is the number of outgoing network connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cell_id": "6b33fda27eb04ef2ad06126f77cc4dfe",
        "deepnote_cell_height": 97,
        "deepnote_cell_type": "code",
        "id": "AAONnAXQ2tgq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def xavier_initialization(input_size, output_size) -> np.ndarray:\n",
        "    \"\"\" Returns a numpy array of initialized weights \"\"\"\n",
        "    bound = np.sqrt(6) / np.sqrt(input_size + output_size)\n",
        "    weights = np.random.uniform(-bound, bound, size=(input_size, output_size))\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "153fa06156c54bdc8e8b10a85a586dd5",
        "deepnote_cell_height": 341.78125,
        "deepnote_cell_type": "markdown",
        "id": "oaSce1El2tgq",
        "tags": []
      },
      "source": [
        "### Feed-Forward Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpzAe_5LI_E"
      },
      "source": [
        "A feed-forward layer applies a linear transformation to the input $x$ using a weight matrix $\\textbf{W}$ and a bias vector $b$:\n",
        "\n",
        "$z = x\\textbf{W}^T+b$\n",
        "\n",
        "Derivatives:\n",
        "$$\n",
        "\\dfrac{dz}{dw_i} = x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\dfrac{dz}{db} = 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\dfrac{dz}{dx_i} = w_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yq1EOJNOVktK"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Input shape of the layer\n",
        "            output_size (int): Output of the layer\n",
        "        \"\"\"\n",
        "        # initialize weights with Xavier intitialization and biases with zeros\n",
        "        self.weights = xavier_initialization(input_size, output_size)\n",
        "        self.biases = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): input to the layer\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "\n",
        "        # Calculate the output\n",
        "        output = ...\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_values, learning_rate):\n",
        "        \"\"\"\n",
        "        Backpropagation\n",
        "\n",
        "        Args:\n",
        "            d_values (float): Derivative of the output\n",
        "            learning_rate (float): Learning rate for gradient descent\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate the derivative with respect to the weight and bias (one with weight and one with bias)\n",
        "        d_weights = ...\n",
        "        d_biases = ...\n",
        "\n",
        "        # Calculate the gradient with respect to the input\n",
        "        d_inputs = ...\n",
        "\n",
        "        # Update the weights and biases using the learning rate and their derivatives\n",
        "        self.weights = ...\n",
        "        self.biases = ...\n",
        "\n",
        "        return d_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaTvv5u_Haba"
      },
      "source": [
        "**Question**: Why do we need to calculate `d_weights`, `d_biases` and `d_inputs`?\n",
        "\n",
        "**Answer**: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5f712a71667a48fba5caf101171168cb",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "KbVJQkvx2tgt",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Adding Nonlinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4f3d994ea8c4a7892f2c1f1f1d71082",
        "deepnote_cell_height": 651.015625,
        "deepnote_cell_type": "markdown",
        "id": "TJx_JZbl2tgt",
        "owner_user_id": "06b28ca6-80fe-4ecd-a509-50438de77bba",
        "tags": []
      },
      "source": [
        "For nonlinearity, you should implement Rectified Linear Unit (ReLU) and apply it between the hidden layers to provide nonlinearity to the network.\n",
        "\n",
        "$$ y = max(0, x) $$\n",
        "\n",
        "When we examine the ReLU behavior, it looks like it is the combination of two different linear functions. This property makes the training easier yet effective since ReLU does not have any learnable parameters as well as easy to apply because of combination of two simple linear functions.\n",
        "\n",
        "\n",
        "\n",
        "<center><figure><img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"450\"/><figcaption>Graph of the ReLU activation function. <a href=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\">Image source</a></figcaption></figure></center>\n",
        "\n",
        "\n",
        "Derivative of ReLU:\n",
        "\n",
        "$\\dfrac{dy}{dx} = 1 $ if $x >= 0$\n",
        "\n",
        "$\\dfrac{dy}{dx} = 0 $ if $x < 0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "9ea233c84766446f966fa64794d2d63f",
        "deepnote_cell_height": 349,
        "deepnote_cell_type": "code",
        "id": "8W-5egab2tgt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ReluActivationFunction():\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        self.x = x\n",
        "        output = ...\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_values):\n",
        "        # calculate the gradients with help of the derivative\n",
        "        d_inputs = ...\n",
        "        return d_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aaab44ed0b0948398d7bc4258dee27ab",
        "deepnote_cell_height": 267.75,
        "deepnote_cell_type": "markdown",
        "id": "jc6g4n0-2tgp",
        "tags": []
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "The perceptron algorithm can't be generalized to MLP, that's why we will now use **backpropagation**.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/LgBzpYD.png\" width=\"400\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6360b3c0360d4193981ac91f621abef8",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0AumYOUd2tgp",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Loss Function: Binary Cross Entropy\n",
        "\n",
        "Backpropagation requires us to have a **loss function**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "52577f99ec9d4cb5918a774edbe97d63",
        "deepnote_cell_height": 89.390625,
        "deepnote_cell_type": "markdown",
        "id": "Z53G5Bwn2tgp",
        "tags": []
      },
      "source": [
        "$$ L = - (y \\times ln(o)+(1-y) \\times ln(1-o)) $$\n",
        "\n",
        "[Derivative](https://www.google.com/search?q=cross+entropy+loss+derivative&sca_esv=6915796dc894fc83&sca_upv=1&rlz=1C5CHFA_enVN752VN752&udm=2&biw=1309&bih=708&sxsrf=ACQVn09fs99X4SFZJk0xmct6PWrepRzpxQ%3A1713181875984&ei=sxQdZuGxO8eE9u8P9oiI6AI&ved=0ahUKEwih15zpk8SFAxVHgv0HHXYEAi0Q4dUDCBA&uact=5&oq=cross+entropy+loss+derivative&gs_lp=Egxnd3Mtd2l6LXNlcnAiHWNyb3NzIGVudHJvcHkgbG9zcyBkZXJpdmF0aXZlMgQQIxgnMgUQABiABDIHEAAYgAQYGEjkBVDRA1jRA3ACeACQAQCYATCgATCqAQExuAEDyAEA-AEBmAIBoAIzmAMAiAYBkgcBMaAHsAM&sclient=gws-wiz-serp#vhid=fKdGq3KS8we6mM&vssid=mosaic):\n",
        "\n",
        "$$\n",
        "\\dfrac{dL}{do} = \\dfrac{-y}{o} + \\dfrac{1-y}{1-o}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "f8f969c517f24b28b485149b9bdfd8c5",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "BtsbaLuL2tgq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class BinaryCrossEntropy():\n",
        "\n",
        "    def forward(self, output, target):\n",
        "\n",
        "        # TODO\n",
        "        # implement Binary Cross-Entrops loss function for output, target\n",
        "\n",
        "        loss = ...\n",
        "\n",
        "        # END TODO\n",
        "        return loss\n",
        "\n",
        "    def backward(self, output, target):\n",
        "        # Calculate the gradient with respect to the output\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "207132d07fec4fe887a8ce578307e95a",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "NHAGXyrR2tgp",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Sigmoid Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b75c8af994a84628af79cb6906e06842",
        "deepnote_cell_height": 97.171875,
        "deepnote_cell_type": "markdown",
        "id": "y-LgMkeC2tgp",
        "tags": []
      },
      "source": [
        "For a binary classification problem, we can use the sigmoid activation function in the output layer which outputs values in the range of 0 and 1. So, for a positive case (class 1), we can interpret $p_1 = \\sigma(o)$ as the probability of that class, while $p_0 = 1 - p_1$ can be seen the probability of the negative case (class 0).\n",
        "\n",
        "**Sigmoid function**:\n",
        "$$\n",
        "\\sigma(x) = \\dfrac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "[Derivative](https://hausetutorials.netlify.app/posts/2019-12-01-neural-networks-deriving-the-sigmoid-derivative/#:~:text=The%20derivative%20of%20the%20sigmoid%20function%20%CF%83(x)%20is%20the,1%E2%88%92%CF%83(x).):\n",
        "$$\n",
        "\\dfrac{d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x))\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "f3ee1efc24284dfc836bb18198f6a1e8",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "p9nIIYj02tgp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return ...\n",
        "\n",
        "class SigmoidActivationFunction():\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        # implement Sigmoid function for the input_data\n",
        "        self.x = x\n",
        "\n",
        "        output = ...\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_values):\n",
        "        # calculate the gradients with help of the derivative\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e731239aef1b43b6a1b58b47a03f0682",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "RJYI8_GE2tgs",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Implementation\n",
        "\n",
        "Now let's put together the components you have implemented so far to our MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "163c21039acf4345856cc2d1fec878d7",
        "deepnote_cell_height": 997,
        "deepnote_cell_type": "code",
        "id": "7kB5OBuF2tgs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MLP_implementation():\n",
        "    def __init__(self,\n",
        "        input_size,\n",
        "        output_size,\n",
        "        hidden_layers,\n",
        "        hidden_layers_size,\n",
        "        hidden_activation_func,\n",
        "        output_activation_function,\n",
        "        loss_function,\n",
        "    ):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_layers_size = hidden_layers_size\n",
        "        self.hidden_activation_func = hidden_activation_func\n",
        "        self.loss_function = loss_function\n",
        "        self.output_activation_function = output_activation_function\n",
        "        self.layers = []\n",
        "\n",
        "        # Initialize hidden layers\n",
        "        for i in range(hidden_layers):\n",
        "            if i == 0:\n",
        "                layer = ...\n",
        "            else:\n",
        "                layer = ...\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        # Initialize output layer\n",
        "        self.output_layer = ...\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        ...\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, d_values, learning_rate):\n",
        "        # Backpropagate through output layer\n",
        "        d_values = self.output_activation_function.backward(d_values)\n",
        "        ...\n",
        "\n",
        "        # Backpropagate through hidden layers\n",
        "        for layer in reversed(self.layers):\n",
        "            ...\n",
        "\n",
        "\n",
        "    def train(self, input_data, targets, learning_rate=1, epochs=1):\n",
        "        for epoch in range(epochs):\n",
        "            random_order = np.random.permutation(np.array(range(len(input_data))))\n",
        "            for i in random_order:\n",
        "                # Forward pass\n",
        "                output = ...\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = ...\n",
        "\n",
        "                # Backward pass\n",
        "                ...\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        output = []\n",
        "        for i in range(len(input_data)):\n",
        "            ...\n",
        "        return np.array(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f3eb7f63f1ef4b76907870481e5dde3e",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0l6UYSco2tgt",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### MLP Inititialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "c463f6ca71c64121a27152c58878f53f",
        "deepnote_cell_height": 79,
        "deepnote_cell_type": "code",
        "id": "oUEAOGUT2tgt",
        "tags": []
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "MLP_implementation.__init__() missing 6 required positional arguments: 'output_size', 'hidden_layers', 'hidden_layers_size', 'hidden_activation_func', 'output_activation_function', and 'loss_function'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize MLP\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m xor_mlp \u001b[38;5;241m=\u001b[39m \u001b[43mMLP_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
            "\u001b[1;31mTypeError\u001b[0m: MLP_implementation.__init__() missing 6 required positional arguments: 'output_size', 'hidden_layers', 'hidden_layers_size', 'hidden_activation_func', 'output_activation_function', and 'loss_function'"
          ]
        }
      ],
      "source": [
        "# Initialize MLP\n",
        "xor_mlp = MLP_implementation(\n",
        "    ...\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "43d7dc2d8c4448f7ac6aaf1c084f695a",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "CyX7sj5Q2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "5f343b3459d246c8a0b8cdd5fc07a6c7",
        "deepnote_cell_height": 115,
        "deepnote_cell_type": "code",
        "id": "hoi1RVs_2tgu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "input_data = ...\n",
        "targets = ...\n",
        "\n",
        "xor_mlp.train(input_data, targets, learning_rate=0.05, epochs=2500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "be53793e58d24e8a847e7819c7c8b3ce",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "JSdR2IVW2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "04807c8322c24ced887c6cb5df3d7f32",
        "deepnote_cell_height": 115,
        "deepnote_cell_type": "code",
        "id": "yKDqNb5f2tgu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test and evaluate your new model as in the previous task\n",
        "# TODO\n",
        "predictions = ...\n",
        "accuracy_value = ...\n",
        "print(accuracy_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mct9npwKG-Md"
      },
      "source": [
        "You will now be able to get to 100% accuracy on the XOR task with MLP!!\n",
        "\n",
        "If you are interested, you can see this [demo](https://lecture-demo.iar.kit.edu/neural-network-demo/) to see how the decision boundaries are found by the MLPs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceADx2coCFh2"
      },
      "source": [
        "## Excercise 3: XOR Task with MLP (using Pytorch)\n",
        "\n",
        "Everything could have been much easier!\n",
        "\n",
        "The excercises so far is only for you to undertand the internal details of training a neural network. In practice, we do not have to implement the forward and backward pass of the common function by hand. All can be taken care of by Pytorch!\n",
        "\n",
        "Look up the Pytorch documentation, and fill in the following blocks of code to build the same MLP with Pytorch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2o1Yh9LFRiW"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phWDZI1JFcLG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "xor_mlp_pytorch = nn.Sequential(\n",
        "    ...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6e49IHFfGx"
      },
      "source": [
        "### Initializing weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSyW-ZnBFlaw"
      },
      "outputs": [],
      "source": [
        "# Init weights\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Apply the initialization to the model\n",
        "xor_mlp_pytorch.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hW18Dv4FnhK"
      },
      "source": [
        "### Loss Function: Binary Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5srvaMyoFvrU"
      },
      "outputs": [],
      "source": [
        "loss_fn = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G488ldnEFxpj"
      },
      "source": [
        "### Optimizer: Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMKoSgccF8jm"
      },
      "outputs": [],
      "source": [
        "optimizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho2rKdqvGAmm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcwMbd2_KSXF"
      },
      "source": [
        "Below we provide you with a simple training loop.\n",
        "\n",
        "For the first two epochs, print out the gradient and the values some weights of the network.\n",
        "\n",
        "**Question**:\n",
        "- Explain what happens after each step in the training loop.\n",
        "- Why do we need `optimizer.zero_grad()` here? When should we NOT use it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUsnqNSoSWp"
      },
      "source": [
        "**Answer**: ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcIT01QxCLPb"
      },
      "outputs": [],
      "source": [
        "# Define our data\n",
        "input_data_tensor = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float)\n",
        "targets_tensor = torch.unsqueeze(\n",
        "    torch.tensor([0,1,1,0], dtype=torch.float), 1\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "epochs = 2500\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "    output = xor_mlp_pytorch(input_data_tensor)\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "\n",
        "    loss = loss_fn(output, targets_tensor)\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "    optimizer.step()\n",
        "    if epoch < 2:\n",
        "        print(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYcyR0S1qEQj"
      },
      "source": [
        "Follow the loss in the backward direction, using its `.grad_fn` attribute too see the computation graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrSFiY6phlAq"
      },
      "outputs": [],
      "source": [
        "print(...)\n",
        "print(...)\n",
        "print(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBD9_kRXGLlx"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbZ6oW3CGU6K"
      },
      "outputs": [],
      "source": [
        "predictions = ...\n",
        "accuracy_value = ...\n",
        "print(accuracy_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "47b0a74bd12b4a3b9a395514d0b7683b",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "P1rqjyq42tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Excercise 4: Iris Dataset 🌷 task with MLP (using Pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8d25765564d04e78b0a65b42ba08d5b4",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "gsMh6WiW2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "Iris is a genus of hundreds of species of flowering plants with showy flowers. The Iris data set consists of 150 samples from three species of Iris which are hard to distinguish (Iris setosa, Iris virginica and Iris versicolor). There are four features from each sample: the length and the width of the sepals and petals, in centimeters. Based on these features, the goal is to predict which species of Iris the sample belongs to.\n",
        "\n",
        "\n",
        "For this exercise, you need to enable GPUs for this notebook:\n",
        "\n",
        "- Navigate to \"**Edit**\" → \"**Notebook Settings**\"\n",
        "- Select GPU from the **Hardware Accelerator** drop-down\n",
        "- You might need to rerun the notebook\n",
        "\n",
        "Next, we'll check if we can connect to the GPU with PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l23EV0A92F1J"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    print('Current device:', torch.cuda.get_device_name(device))\n",
        "else:\n",
        "    print('Failed to find GPU. Will use CPU.')\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d853a4f5816e49a6ac838230e0655087",
        "deepnote_cell_height": 62,
        "deepnote_cell_type": "markdown",
        "id": "lIB2XdFb2tgv",
        "tags": []
      },
      "source": [
        "###  Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "263a53c7e9754ea488149500a7842994",
        "deepnote_cell_height": 187,
        "deepnote_cell_type": "code",
        "id": "eMgXi4pF2tgv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "num_classes = 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEtvK8_Ilg8N"
      },
      "source": [
        "Process the data.\n",
        "\n",
        "**Question**: Is there anything we need to do with the default target? Why?\n",
        "\n",
        "**Answer**: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTJGr3RVltFT"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and test dataset\n",
        "# TODO\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# Process the data\n",
        "# TODO\n",
        "y_train_one_hot = ...\n",
        "y_test_one_hot = ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "379f8681cb0f4d54bdb2a83938685764",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "TaCmfbHE2tgw",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a9ab559f86de48bd8f2ee8f13a64fbda",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "rT-aJ-3f2tgw",
        "tags": []
      },
      "source": [
        "We will again use an MLP for this task (with Pytorch).\n",
        "\n",
        "Intitialize a model with **4 hidden layers** and a **hidden layer size of 768**.\n",
        "\n",
        "**Question**: Is there any else we should change when building the MLP to fit this task?\n",
        "\n",
        "**Hint**: it is no longer a binary classification problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QheJG1r0wp-s"
      },
      "source": [
        "**Answer:** ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuuDwsJ77J1l"
      },
      "outputs": [],
      "source": [
        "# Defining the model\n",
        "xor_mlp_pytorch = nn.Sequential(\n",
        "    ...\n",
        ")\n",
        "\n",
        "# Apply the initialization to the model\n",
        "xor_mlp_pytorch.apply(init_weights)\n",
        "\n",
        "# Defining loss function: Cross Entropy Loss\n",
        "loss_fn = ...\n",
        "\n",
        "# Defining optimizer: Stochastic gradient descent\n",
        "optimizer = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "651c8f0d9ea44ddc9ab7ad6aa7183498",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "lPRWF1BF2tgw",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training\n",
        "\n",
        "As you have learnt from the lecture, we can speed up the training process by **batching** and using **GPUs**. Modify the following code for batching and GPUs.\n",
        "\n",
        "You can also run the code before and after you make changes to see the speed up gain from batching and using GPU.\n",
        "\n",
        "**Hints**: You can make use of Pytorch's `DataLoader`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojseAt7t3Is1"
      },
      "source": [
        "**Question**: Report the execution time with and without GPU and batching.\n",
        "\n",
        "**Answer**: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOfkgN_p3WaA"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = xor_mlp_pytorch(X_train)\n",
        "    loss = loss_fn(output, y_train_one_hot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e383e8479b24456fb456a2b163651d8e",
        "deepnote_cell_height": 62,
        "deepnote_cell_type": "markdown",
        "id": "NGxuBpxx2tgw",
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "12d63c77198848e08774450159eb4f5b",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "kRnjFbCD2tgx",
        "tags": []
      },
      "source": [
        "Show the overall accuracy of our model on the test dataset. Use the existing `accuracy` function that you implemented earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "f3491bbddfce45f3bbcac76872d99a7a",
        "deepnote_cell_height": 223,
        "deepnote_cell_type": "code",
        "id": "9ujmW5mT2tgx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "predictions = ...\n",
        "accuracy_value = ...\n",
        "print(accuracy_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "157c73719d994eb1a61b10d78034ae37",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "qK4VI1br2tgx",
        "tags": []
      },
      "source": [
        "Print the confusion matrix using `sklearn.metrics.confusion_matrix`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ce02ae5b924440b398b5ca899d94a6f6",
        "deepnote_cell_height": 97,
        "deepnote_cell_type": "code",
        "id": "8Tr40s4f2tgx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cd41f0bd4689449b85001f94469b6dfb",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "9gtdQkiC2tgx",
        "tags": []
      },
      "source": [
        "**Question**: Now also look at the confusion matrix, what can you conclude from it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9bece33cdd404cd3bbf2c11a7ea557d5",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "9gykcH6x2tgx",
        "tags": []
      },
      "source": [
        "**Answer**: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40YpVK5KnxcZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "50c68a69-c321-4e24-b142-cdaa47048c60",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
