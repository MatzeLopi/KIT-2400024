{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatzeLopi/KIT-2400024/blob/main/DLNN_SS24_Praktikum1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b471288db3b44cfb992b6c07f7c3fd83",
        "deepnote_cell_type": "text-cell-h1",
        "formattedRanges": [],
        "id": "w6spdQYQ2tgd",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "# Praktikum 1 - Simple Neural Network with Numpy and Pytorch\n",
        "\n",
        "Note: the praktikums are for your own practice. They will **not be graded**!\n",
        "\n",
        "You have around one week to work on it. Then we will go over the solutions together in the praktikum time slots!\n",
        "\n",
        "Remember to make a copy of this notebook to your own Colab. Changes made directly here will not be stored!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7b2ab899b012471f80f00f1ff3b7992a",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "xWJ_eyKS2tge",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "In this praktikum, we will build a perceptron model and a Multi-Layer Perceptron (MLP) model **from scratch** with numpy. This is intended to help you understand in details the internal working of neural networks.\n",
        "\n",
        "Next, we will proceed with building an MLP with [**Pytorch**](https://pytorch.org/get-started/locally/), which is a widely-used framework for building deep learning models.\n",
        "\n",
        "Our first challenge is solving the **XOR task** that you've seen in the lecture, before we move to a slightly more complex problem, namely the **Iris dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dfb46b3d43444b14961ecd9967845ea1",
        "deepnote_cell_height": 141.953125,
        "deepnote_cell_type": "markdown",
        "id": "pOIoDJv_2tgf",
        "tags": []
      },
      "source": [
        "**Notice**: Whenenver you see an ellipsis `...` or TODO comment, you're supposed to insert code or text answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "10a3e57eb4844285b70b25bcd41c834e",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "M8QD3-q02tgg",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Excercise 1: XOR Task with Single Perceptron (from scratch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "84d575c151ba447192c5bc8d6bf696e1",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "B4XIvrKj2tgh",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "XOR (exclusive OR) is a logic function that gives 1 as an output when the number of true inputs is odd, otherwise it outputs a 0. Our goal is to model this function using neurons. We'll start with a single neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c9900820a8204251a55585f98126c98e",
        "deepnote_cell_height": 276.84375,
        "deepnote_cell_type": "markdown",
        "id": "ekh7hOIk2tgh",
        "tags": []
      },
      "source": [
        "<center><img src=\"https://www.xplore-dna.net/pluginfile.php/286/mod_page/content/21/Tabelle%20-%20XOR.png\" width=\"250\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "eeef0f122a2e45be9ab17634e036b17f",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "RS83rXR82tgi",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "Let's start with importing some necessary dependencies that we will need throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "cell_id": "95e9b1a1f9494ea89f65aee06be516dc",
        "deepnote_cell_height": 61,
        "deepnote_cell_type": "code",
        "id": "-nrjhMBO2tgj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "39505416089744699256a5844dd99d19",
        "deepnote_cell_height": 252.9375,
        "deepnote_cell_type": "markdown",
        "id": "dlGxHaM72tgk",
        "tags": []
      },
      "source": [
        "In the first part of this exercise you'll build a perceptron, a single neuron, that takes two binary input values and returns a binary output value.\n",
        "\n",
        "<center><img src=\"https://i.stack.imgur.com/eBSki.jpg\" width=\"280\" />\n",
        "\n",
        "<center><img src=\"\" width=\"280\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "816aceffd3b045029c7d82becc35489e",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "I7PmyXz92tgk",
        "tags": []
      },
      "source": [
        "Perceptron can be seen as a single neuron, mapping an input $\\textbf{x}$ to an output $o$ using weights $\\textbf{w}$ and a bias $b$. $\\cdot$ is the dot product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "11a6fcabddb143599d0b7bc55f5cc50c",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "7UjfCaeq2tgl",
        "tags": []
      },
      "source": [
        "$o = \\textbf{w}\\cdot \\textbf{x}+b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "59d21d8a14cf4ffab7daf30a94c91be8",
        "deepnote_cell_height": 110.78125,
        "deepnote_cell_type": "markdown",
        "id": "QW5c6tT52tgl",
        "tags": []
      },
      "source": [
        "#### Perceptron Update Rule\n",
        "\n",
        "\n",
        "Perceptron Update Rule is a process that is specific to the training of a single-perceptron model, which we can apply to binary classification problems. This process [has been proven to converge](https://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf) if the data is linearly seperatable and the learning rate is small enough.\n",
        "\n",
        "\n",
        "Let's use it here to have a first baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fe3ac47eb7134fbd9e92b9b887dcda12",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "5dNqpPdd2tgl",
        "tags": []
      },
      "source": [
        "For classification problems $0>o$ is interpreted as class 1, and $o<0$ is interpreted as class 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6454ee77e7e4411f8d341729e17e90a5",
        "deepnote_cell_height": 262.734375,
        "deepnote_cell_type": "markdown",
        "id": "CWqEU0Ds2tgl",
        "tags": []
      },
      "source": [
        "For updating the associated weights, we can use the following update rule:\n",
        "\n",
        "$w_i = w_i + \\nabla w_i$\n",
        "\n",
        "where\n",
        "\n",
        "$\\nabla w_i = \\eta(t-o)x_i$\n",
        "\n",
        "- $t$ is the target\n",
        "- $o$ is the output\n",
        "- $\\eta$ is the learning rate (a small constant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "89280b788ec34866ab8f8ead1cc686c3",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "oCy6z6Ig2tgm",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Implementation of a Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "cell_id": "82eb15fa556f4017824bfc25bdfa9b71",
        "deepnote_cell_height": 871,
        "deepnote_cell_type": "code",
        "id": "eNs7zHyT2tgm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def step_function(x) -> int:\n",
        "    \"\"\" Step function that returns 1 if x >= 0, 0 otherwise\"\"\"\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "class Perceptron():\n",
        "    \"\"\" Implementation of a Perceptron following the above instructions\n",
        "\n",
        "    Attributes:\n",
        "        neuron_weights (np.array): Weights of the perceptron\n",
        "        bias (int): Bias of the perceptron\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\" Initialize the perceptron weights and bias\"\"\"\n",
        "        # TODO:\n",
        "        # Initialize weights\n",
        "        # For perceptrons, it's possible to initialize all weights with 0\n",
        "\n",
        "        self.neuron_weights = np.zeros(2)\n",
        "        self.bias = 0\n",
        "\n",
        "    def forward_pass(self, x:np.array,activation_func: Callable = step_function) -> np.array:\n",
        "        \"\"\"Forward pass of the perceptron\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Input from the previous layer / input data\n",
        "\n",
        "        Returns:\n",
        "            np.array: Weighted sum of the input data + bias\n",
        "        \"\"\"\n",
        "        output = sum(self.neuron_weights * x) + self.bias\n",
        "\n",
        "        return output if activation_func is None else activation_func(output)\n",
        "\n",
        "    def perceptron_update_rule(self, target:np.array, prediction:np.array, x:np.array, learning_rate=1) -> None:\n",
        "        \"\"\" Perform update of the perceptron following the rule defined above\n",
        "\n",
        "        Args:\n",
        "            target (np.array): Target value\n",
        "            prediction (np.array): Predicted value\n",
        "            x (np.array): Input data\n",
        "            learning_rate (int, optional): Learning rate. Defaults to 1.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "\n",
        "        \"\"\"\n",
        "        new_weights = self.neuron_weights.copy()\n",
        "        for index, _ in enumerate(self.neuron_weights):\n",
        "\n",
        "            weight_delta = learning_rate * ((target - prediction) * x[index])\n",
        "            new_weights[index] += weight_delta\n",
        "\n",
        "\n",
        "        self.neuron_weights = new_weights\n",
        "\n",
        "    def train(self, input_data:np.ndarray, targets:np.ndarray):\n",
        "        \"\"\" Training loop for the perceptron\n",
        "        \n",
        "        Args:\n",
        "            input_data: Multi-dimensional array that contains all inputs\n",
        "            targets: Array that contains all targets\n",
        "        \"\"\"\n",
        "        for data, target in zip(input_data, targets):\n",
        "            prediction = self.forward_pass(data,step_function)\n",
        "            print(data, prediction, target)\n",
        "            self.perceptron_update_rule(target, prediction, data)\n",
        "\n",
        "    def inference(self, input_data:np.ndarray):\n",
        "        \"\"\" Inference of the perceptron\n",
        "\n",
        "        Args:\n",
        "            input_data (_type_): _description_\n",
        "\n",
        "        Returns:\n",
        "            _type_: _description_\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        outputs = []\n",
        "        for x in input_data:\n",
        "            outputs.append(self.forward_pass(x,step_function))\n",
        "\n",
        "        return np.array(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "75e4622dce1a4d39b1734879909d04d2",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "5kpPuTNY2tgn",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "cell_id": "fdac76e29da047579c02f7c61cec4ffc",
        "deepnote_cell_height": 223,
        "deepnote_cell_type": "code",
        "id": "k7UXXkGL2tgn",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0] 1 0\n",
            "[1, 0] 1 1\n",
            "[0, 1] 1 1\n",
            "[1, 1] 1 0\n"
          ]
        }
      ],
      "source": [
        "perceptron = Perceptron()\n",
        "\n",
        "# TODO\n",
        "# Define input data and targets\n",
        "input_data = [[0,0],[1,0],[0,1],[1,1]]\n",
        "targets = [0,1,1,0]\n",
        "\n",
        "perceptron.train(input_data, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c22027433597480481df2737709639dd",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "TD9gtzyL2tgn",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "cell_id": "c454d52152ce428c81d51ad9f34fd970",
        "deepnote_cell_height": 151,
        "deepnote_cell_type": "code",
        "id": "agv_oJ382tgn",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "\n",
        "predictions = perceptron.inference(input_data)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "85b73e5dbdd14488b836e0dad804b0a3",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "DcMjb5zH2tgo",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e76ebc19dbad44988015c11187bd7969",
        "deepnote_cell_height": 243.34375,
        "deepnote_cell_type": "markdown",
        "id": "ETEW4Rhp2tgo",
        "tags": []
      },
      "source": [
        "For evaluation, we will need to consider appropriate metrics. For classification tasks, **accuracy** is one of the most common metrics.\n",
        "\n",
        "It is defined as:\n",
        "\n",
        "$\\textrm{Accuracy}=\\frac{1}{N}\\sum_i^N1(y_i=\\hat{y}_i)$\n",
        "\n",
        "where $y$ is an array of our target values, and $\\hat{y}$ is an array of our predictions.\n",
        "\n",
        "For accuracy, if outputs are probabilities, there needs to be a threshold for transforming logit predictions to binary `(0,1)` predictions. We will set this threshold to `0.5`. For our perceptron this is not needed, since we already output binary values, however, we will use the `accuracy` function later on, so the predictions should be considered to be probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "kpPwA9EeGm_Q"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions: np.ndarray, targets: np.ndarray, threshold=0.5) -> float:\n",
        "    \"\"\" Function to evaluate the accuracy of the model\n",
        "\n",
        "    Args:\n",
        "        predictions (np.ndarray): Predictions of the model\n",
        "        targets (np.ndarray): Actual Targets\n",
        "        threshold (float, optional): Threshold to convert percentage to binary. Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        float: accuracy of the model\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(predictions) == len(targets)\n",
        "\n",
        "    accuracy_value = 0\n",
        "    for prediction, target in zip(predictions, targets):\n",
        "        if prediction >= threshold:\n",
        "            prediction = 1\n",
        "        else:\n",
        "            prediction = 0\n",
        "        \n",
        "        if prediction == target:\n",
        "            accuracy_value += 1\n",
        "\n",
        "    return accuracy_value / len(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "cell_id": "65abd9e35d404aa5af91b1ab7d74147b",
        "deepnote_cell_height": 259,
        "deepnote_cell_type": "code",
        "id": "xJCPWPYj2tgo",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25\n"
          ]
        }
      ],
      "source": [
        "\n",
        "accuracy_value = accuracy(predictions, targets) \n",
        "print(accuracy_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSQ8rFEB5Q7"
      },
      "source": [
        "You will see that it is not possible to get to 100% accuracy, since XOR is not a linear-separatable problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ed9c69e1f9d7460b89f6899e938a68bf",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "9zwDNf042tgs",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Excercise 2: XOR Task with MLP (from scratch)\n",
        "\n",
        "As mentioned in the lecture, unlike a single perceptron, Multi-Layer Perceptron (MLP) can deal with problems that are non-linearly-separatable like XOR.\n",
        "\n",
        "Now we will try to implement an MLP with 3 hidden layers and a hidden dimension of 3. We will also add an activiation function to introduce nonlinearity in our hidden layers.\n",
        "\n",
        "<img src=\"https://i.imgur.com/IUQ05Ol.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "df9ec443c0284dc4a52c55b8caa88358",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "UQdPsvFa2tgq",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Initializing Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c251b1998be74c5fbc41cd1d47edaa88",
        "deepnote_cell_height": 101.46875,
        "deepnote_cell_type": "markdown",
        "id": "lV-m23SR2tgq",
        "tags": []
      },
      "source": [
        "Xavier intitialization is commonly used to initialize the weights of a network. It is a random uniform distribution that’s bounded between $\\pm\\frac{\\sqrt{6}}{\\sqrt{n_i+n_{i+1}}}$ where $n_i$ is the number of incoming network connections, and $n_{i+1}$ is the number of outgoing network connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "cell_id": "6b33fda27eb04ef2ad06126f77cc4dfe",
        "deepnote_cell_height": 97,
        "deepnote_cell_type": "code",
        "id": "AAONnAXQ2tgq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def xavier_initialization(input_size, output_size) -> np.ndarray:\n",
        "    \"\"\" Returns a numpy array of initialized weights \"\"\"\n",
        "    bound = np.sqrt(6) / np.sqrt(input_size + output_size)\n",
        "    weights = np.random.uniform(-bound, bound, size=(input_size, output_size))\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "153fa06156c54bdc8e8b10a85a586dd5",
        "deepnote_cell_height": 341.78125,
        "deepnote_cell_type": "markdown",
        "id": "oaSce1El2tgq",
        "tags": []
      },
      "source": [
        "### Feed-Forward Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpzAe_5LI_E"
      },
      "source": [
        "A feed-forward layer applies a linear transformation to the input $x$ using a weight matrix $\\textbf{W}$ and a bias vector $b$:\n",
        "\n",
        "$z = x\\textbf{W}^T+b$\n",
        "\n",
        "Derivatives:\n",
        "$$\n",
        "\\dfrac{dz}{dw_i} = x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\dfrac{dz}{db} = 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\dfrac{dz}{dx_i} = w_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Yq1EOJNOVktK"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer():\n",
        "    \"\"\" Implementation of a feedforward layer\n",
        "\n",
        "    Attributes:\n",
        "        weights (np.ndarray): Weights of the layer. Matrix of shape (input_size, output_size)\n",
        "        biases (np.ndarray): Biases of the layer. Vector of shape (1, output_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size:int, output_size:int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): Input shape of the layer\n",
        "            output_size (int): Output of the  \n",
        "        \"\"\"\n",
        "        # initialize weights with Xavier intitialization and biases with zeros\n",
        "        self.weights = xavier_initialization(input_size, output_size)\n",
        "        self.biases = np.zeros(output_size)\n",
        "\n",
        "    def forward(self, x) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): Input to the layer\n",
        "        \"\"\"\n",
        "        self.x = np.array(x)\n",
        "\n",
        "        # Calculate the output by applying an activation function\n",
        "        output = np.dot(self.weights.T, self.x) + self.biases\n",
        "        print(f\"Output of the network, {output}\")\n",
        "\n",
        "        assert len(output) == self.output_size\n",
        "        print(f\"Output of the network\")\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_values, learning_rate):\n",
        "        \"\"\"\n",
        "        Backpropagation of the layer\n",
        "\n",
        "        Args:\n",
        "            d_values (np.ndarray): Derivative of the loss with respect to the output of the layer\n",
        "            learning_rate (float): Learning rate for gradient descent\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Derivative of the loss with respect to the input of the layer\n",
        "        \"\"\"\n",
        "        # Calculate the derivative with respect to the weights\n",
        "        d_weights = np.dot(self.x.T, d_values)\n",
        "\n",
        "        # Calculate the derivative with respect to the biases\n",
        "        d_biases = np.sum(d_values, axis=0, keepdims=True)\n",
        "\n",
        "        # Calculate the derivative with respect to the input\n",
        "        d_inputs = np.dot(d_values, self.weights.T)\n",
        "\n",
        "        # Update the weights and biases using the learning rate and their derivatives\n",
        "        self.weights -= learning_rate * d_weights\n",
        "        self.biases -= learning_rate * d_biases\n",
        "\n",
        "        return d_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaTvv5u_Haba"
      },
      "source": [
        "**Question**: Why do we need to calculate `d_weights`, `d_biases` and `d_inputs`?\n",
        "\n",
        "**Answer**: To enable the network to learn using back propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5f712a71667a48fba5caf101171168cb",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "KbVJQkvx2tgt",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Adding Nonlinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4f3d994ea8c4a7892f2c1f1f1d71082",
        "deepnote_cell_height": 651.015625,
        "deepnote_cell_type": "markdown",
        "id": "TJx_JZbl2tgt",
        "owner_user_id": "06b28ca6-80fe-4ecd-a509-50438de77bba",
        "tags": []
      },
      "source": [
        "For nonlinearity, you should implement Rectified Linear Unit (ReLU) and apply it between the hidden layers to provide nonlinearity to the network.\n",
        "\n",
        "$$ y = max(0, x) $$\n",
        "\n",
        "When we examine the ReLU behavior, it looks like it is the combination of two different linear functions. This property makes the training easier yet effective since ReLU does not have any learnable parameters as well as easy to apply because of combination of two simple linear functions.\n",
        "\n",
        "\n",
        "\n",
        "<center><figure><img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"450\"/><figcaption>Graph of the ReLU activation function. <a href=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\">Image source</a></figcaption></figure></center>\n",
        "\n",
        "\n",
        "Derivative of ReLU:\n",
        "\n",
        "$\\dfrac{dy}{dx} = 1 $ if $x >= 0$\n",
        "\n",
        "$\\dfrac{dy}{dx} = 0 $ if $x < 0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "cell_id": "9ea233c84766446f966fa64794d2d63f",
        "deepnote_cell_height": 349,
        "deepnote_cell_type": "code",
        "id": "8W-5egab2tgt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ReluActivationFunction():\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        output = max(x)\n",
        "        return output if output > 0 else 0\n",
        "\n",
        "    def backward(self, d_values):\n",
        "        # Calculate the derivative of the ReLU activation function\n",
        "        derivative = 1 if self.x >= 0 else 0\n",
        "        d_output = d_values * derivative\n",
        "        return d_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aaab44ed0b0948398d7bc4258dee27ab",
        "deepnote_cell_height": 267.75,
        "deepnote_cell_type": "markdown",
        "id": "jc6g4n0-2tgp",
        "tags": []
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "The perceptron algorithm can't be generalized to MLP, that's why we will now use **backpropagation**.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/LgBzpYD.png\" width=\"400\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6360b3c0360d4193981ac91f621abef8",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0AumYOUd2tgp",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Loss Function: Binary Cross Entropy\n",
        "\n",
        "Backpropagation requires us to have a **loss function**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "52577f99ec9d4cb5918a774edbe97d63",
        "deepnote_cell_height": 89.390625,
        "deepnote_cell_type": "markdown",
        "id": "Z53G5Bwn2tgp",
        "tags": []
      },
      "source": [
        "$$ L = - (y \\times ln(o)+(1-y) \\times ln(1-o)) $$\n",
        "\n",
        "[Derivative](https://www.google.com/search?q=cross+entropy+loss+derivative&sca_esv=6915796dc894fc83&sca_upv=1&rlz=1C5CHFA_enVN752VN752&udm=2&biw=1309&bih=708&sxsrf=ACQVn09fs99X4SFZJk0xmct6PWrepRzpxQ%3A1713181875984&ei=sxQdZuGxO8eE9u8P9oiI6AI&ved=0ahUKEwih15zpk8SFAxVHgv0HHXYEAi0Q4dUDCBA&uact=5&oq=cross+entropy+loss+derivative&gs_lp=Egxnd3Mtd2l6LXNlcnAiHWNyb3NzIGVudHJvcHkgbG9zcyBkZXJpdmF0aXZlMgQQIxgnMgUQABiABDIHEAAYgAQYGEjkBVDRA1jRA3ACeACQAQCYATCgATCqAQExuAEDyAEA-AEBmAIBoAIzmAMAiAYBkgcBMaAHsAM&sclient=gws-wiz-serp#vhid=fKdGq3KS8we6mM&vssid=mosaic):\n",
        "\n",
        "$$\n",
        "\\dfrac{dL}{do} = \\dfrac{-y}{o} + \\dfrac{1-y}{1-o}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "cell_id": "f8f969c517f24b28b485149b9bdfd8c5",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "BtsbaLuL2tgq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class BinaryCrossEntropy():\n",
        "\n",
        "    def forward(self, output, target):\n",
        "\n",
        "        # TODO\n",
        "        # implement Binary Cross-Entrops loss function for output, target\n",
        "\n",
        "        loss = -(target * np.log(output) + (1 - target) * np.log(1 - output))\n",
        "\n",
        "        assert loss > 0\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, output, target):\n",
        "        # Calculate the gradient with respect to the output\n",
        "        return (- target / output) + (1 - target) / (1 - output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "207132d07fec4fe887a8ce578307e95a",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "NHAGXyrR2tgp",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Sigmoid Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b75c8af994a84628af79cb6906e06842",
        "deepnote_cell_height": 97.171875,
        "deepnote_cell_type": "markdown",
        "id": "y-LgMkeC2tgp",
        "tags": []
      },
      "source": [
        "For a binary classification problem, we can use the sigmoid activation function in the output layer which outputs values in the range of 0 and 1. So, for a positive case (class 1), we can interpret $p_1 = \\sigma(o)$ as the probability of that class, while $p_0 = 1 - p_1$ can be seen the probability of the negative case (class 0).\n",
        "\n",
        "**Sigmoid function**:\n",
        "$$\n",
        "\\sigma(x) = \\dfrac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "[Derivative](https://hausetutorials.netlify.app/posts/2019-12-01-neural-networks-deriving-the-sigmoid-derivative/#:~:text=The%20derivative%20of%20the%20sigmoid%20function%20%CF%83(x)%20is%20the,1%E2%88%92%CF%83(x).):\n",
        "$$\n",
        "\\dfrac{d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x))\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "cell_id": "f3ee1efc24284dfc836bb18198f6a1e8",
        "deepnote_cell_height": 295,
        "deepnote_cell_type": "code",
        "id": "p9nIIYj02tgp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class SigmoidActivationFunction():\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        # implement Sigmoid function for the input_data\n",
        "        self.x = x\n",
        "\n",
        "        output = sigmoid(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, d_values):\n",
        "        # calculate the gradients with help of the derivative\n",
        "        return sigmoid(d_values) * (1 - sigmoid(d_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e731239aef1b43b6a1b58b47a03f0682",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "RJYI8_GE2tgs",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Implementation\n",
        "\n",
        "Now let's put together the components you have implemented so far to our MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "cell_id": "163c21039acf4345856cc2d1fec878d7",
        "deepnote_cell_height": 997,
        "deepnote_cell_type": "code",
        "id": "7kB5OBuF2tgs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MLP_implementation():\n",
        "    def __init__(self,\n",
        "        input_size,\n",
        "        output_size,\n",
        "        hidden_layers,\n",
        "        hidden_layers_size,\n",
        "        hidden_activation_func,\n",
        "        output_activation_function,\n",
        "        loss_function,\n",
        "    ):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_layers_size = hidden_layers_size\n",
        "        self.hidden_activation_func = hidden_activation_func\n",
        "        self.loss_function = loss_function\n",
        "        self.output_activation_function = output_activation_function\n",
        "        self.layers = []\n",
        "\n",
        "        # Initialize hidden layers\n",
        "        for i in range(hidden_layers):\n",
        "            if i == 0:\n",
        "                layer = FeedForwardLayer(input_size, hidden_layers_size)\n",
        "            else:\n",
        "                layer = FeedForwardLayer(hidden_layers_size, hidden_layers_size)\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        # Initialize output layer\n",
        "        self.output_layer = FeedForwardLayer(hidden_layers_size, output_size)\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "            x = self.hidden_activation_func.forward(x)\n",
        "        \n",
        "        output = self.output_layer.forward(x)\n",
        "        output = self.output_activation_function.forward(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, d_values, learning_rate):\n",
        "        # Backpropagate through output layer\n",
        "        d_values = self.output_activation_function.backward(d_values)\n",
        "        self.output_layer.backward(d_values, learning_rate)\n",
        "\n",
        "        # Backpropagate through hidden layers\n",
        "        for layer in reversed(self.layers):\n",
        "            d_values = layer.backward(d_values, learning_rate)\n",
        "\n",
        "    def train(self, input_data, targets, learning_rate=1, epochs=1):\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            total_loss = 0.0\n",
        "            random_order = np.random.permutation(len(input_data))\n",
        "            for i in random_order:\n",
        "                # Forward pass\n",
        "                output = self.forward_pass(input_data[i])\n",
        "                \n",
        "                # Calculate loss\n",
        "                loss = self.loss_function.forward(output, targets[i])\n",
        "                total_loss += loss\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward_pass(self.loss_function.backward(output, targets[i]), learning_rate)\n",
        "\n",
        "            print(f\"Average Loss: {total_loss / len(input_data)}\")\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        output = []\n",
        "        for data in input_data:\n",
        "            output.append(self.forward_pass(data))\n",
        "            \n",
        "        return np.array(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f3eb7f63f1ef4b76907870481e5dde3e",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "0l6UYSco2tgt",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### MLP Inititialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "cell_id": "c463f6ca71c64121a27152c58878f53f",
        "deepnote_cell_height": 79,
        "deepnote_cell_type": "code",
        "id": "oUEAOGUT2tgt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize MLP\n",
        "xor_mlp = MLP_implementation(\n",
        "    input_size=2,\n",
        "    output_size=1,\n",
        "    hidden_layers=3,\n",
        "    hidden_layers_size=3,\n",
        "    hidden_activation_func=ReluActivationFunction(),\n",
        "    output_activation_function=SigmoidActivationFunction(),\n",
        "    loss_function=BinaryCrossEntropy(),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "43d7dc2d8c4448f7ac6aaf1c084f695a",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "CyX7sj5Q2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "cell_id": "5f343b3459d246c8a0b8cdd5fc07a6c7",
        "deepnote_cell_height": 115,
        "deepnote_cell_type": "code",
        "id": "hoi1RVs_2tgu",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2500\n",
            "Input of the network, (2,)\n",
            "Weights: (2, 3)\n",
            "Biases: (3,)\n",
            "[0 1]\n",
            "[[-0.50099212 -0.82075324 -0.46263879]\n",
            " [-1.00010372  0.13515638 -0.8487896 ]]\n",
            "[0. 0. 0.]\n",
            "Output of the network, [-1.00010372  0.13515638 -0.8487896 ]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'FeedForwardLayer' object has no attribute 'output_size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[142], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m      3\u001b[0m targets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[43mxor_mlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[140], line 55\u001b[0m, in \u001b[0;36mMLP_implementation.train\u001b[1;34m(self, input_data, targets, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     52\u001b[0m random_order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mlen\u001b[39m(input_data))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m random_order:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function\u001b[38;5;241m.\u001b[39mforward(output, targets[i])\n",
            "Cell \u001b[1;32mIn[140], line 31\u001b[0m, in \u001b[0;36mMLP_implementation.forward_pass\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 31\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_activation_func\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     34\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer\u001b[38;5;241m.\u001b[39mforward(x)\n",
            "Cell \u001b[1;32mIn[136], line 38\u001b[0m, in \u001b[0;36mFeedForwardLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput of the network, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput of the network\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'FeedForwardLayer' object has no attribute 'output_size'"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "input_data = [[0,0],[1,0],[0,1],[1,1]]\n",
        "targets = [0,1,1,0]\n",
        "\n",
        "xor_mlp.train(input_data, targets, learning_rate=0.05, epochs=2500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "be53793e58d24e8a847e7819c7c8b3ce",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "JSdR2IVW2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "04807c8322c24ced887c6cb5df3d7f32",
        "deepnote_cell_height": 115,
        "deepnote_cell_type": "code",
        "id": "yKDqNb5f2tgu",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[150], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m xor_mlp\u001b[38;5;241m.\u001b[39minference(input_data)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[1;32m----> 5\u001b[0m accuracy_value \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_value)\n",
            "Cell \u001b[1;32mIn[134], line 13\u001b[0m, in \u001b[0;36maccuracy\u001b[1;34m(predictions, targets, threshold)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(predictions: np\u001b[38;5;241m.\u001b[39mndarray, targets: np\u001b[38;5;241m.\u001b[39mndarray, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Function to evaluate the accuracy of the model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m        float: accuracy of the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(targets)\n\u001b[0;32m     15\u001b[0m     accuracy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, targets):\n",
            "\u001b[1;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Test and evaluate your new model as in the previous task\n",
        "# TODO\n",
        "predictions = xor_mlp.inference(input_data)\n",
        "print(predictions)\n",
        "accuracy_value = accuracy(predictions, targets) \n",
        "print(accuracy_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mct9npwKG-Md"
      },
      "source": [
        "You will now be able to get to 100% accuracy on the XOR task with MLP!!\n",
        "\n",
        "If you are interested, you can see this [demo](https://lecture-demo.iar.kit.edu/neural-network-demo/) to see how the decision boundaries are found by the MLPs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceADx2coCFh2"
      },
      "source": [
        "## Excercise 3: XOR Task with MLP (using Pytorch)\n",
        "\n",
        "Everything could have been much easier!\n",
        "\n",
        "The excercises so far is only for you to undertand the internal details of training a neural network. In practice, we do not have to implement the forward and backward pass of the common function by hand. All can be taken care of by Pytorch!\n",
        "\n",
        "Look up the Pytorch documentation, and fill in the following blocks of code to build the same MLP with Pytorch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2o1Yh9LFRiW"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phWDZI1JFcLG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "xor_mlp_pytorch = nn.Sequential(\n",
        "    ...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6e49IHFfGx"
      },
      "source": [
        "### Initializing weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSyW-ZnBFlaw"
      },
      "outputs": [],
      "source": [
        "# Init weights\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Apply the initialization to the model\n",
        "xor_mlp_pytorch.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hW18Dv4FnhK"
      },
      "source": [
        "### Loss Function: Binary Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5srvaMyoFvrU"
      },
      "outputs": [],
      "source": [
        "loss_fn = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G488ldnEFxpj"
      },
      "source": [
        "### Optimizer: Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMKoSgccF8jm"
      },
      "outputs": [],
      "source": [
        "optimizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho2rKdqvGAmm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcwMbd2_KSXF"
      },
      "source": [
        "Below we provide you with a simple training loop.\n",
        "\n",
        "For the first two epochs, print out the gradient and the values some weights of the network.\n",
        "\n",
        "**Question**:\n",
        "- Explain what happens after each step in the training loop.\n",
        "- Why do we need `optimizer.zero_grad()` here? When should we NOT use it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUsnqNSoSWp"
      },
      "source": [
        "**Answer**: ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcIT01QxCLPb"
      },
      "outputs": [],
      "source": [
        "# Define our data\n",
        "input_data_tensor = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float)\n",
        "targets_tensor = torch.unsqueeze(\n",
        "    torch.tensor([0,1,1,0], dtype=torch.float), 1\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "epochs = 2500\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "    output = xor_mlp_pytorch(input_data_tensor)\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "\n",
        "    loss = loss_fn(output, targets_tensor)\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    if epoch < 2:\n",
        "        print(...)\n",
        "\n",
        "    optimizer.step()\n",
        "    if epoch < 2:\n",
        "        print(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYcyR0S1qEQj"
      },
      "source": [
        "Follow the loss in the backward direction, using its `.grad_fn` attribute too see the computation graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrSFiY6phlAq"
      },
      "outputs": [],
      "source": [
        "print(...)\n",
        "print(...)\n",
        "print(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBD9_kRXGLlx"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbZ6oW3CGU6K"
      },
      "outputs": [],
      "source": [
        "predictions = ...\n",
        "accuracy_value = ...\n",
        "print(accuracy_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "47b0a74bd12b4a3b9a395514d0b7683b",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "P1rqjyq42tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "## Excercise 4: Iris Dataset 🌷 task with MLP (using Pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8d25765564d04e78b0a65b42ba08d5b4",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "gsMh6WiW2tgu",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "Iris is a genus of hundreds of species of flowering plants with showy flowers. The Iris data set consists of 150 samples from three species of Iris which are hard to distinguish (Iris setosa, Iris virginica and Iris versicolor). There are four features from each sample: the length and the width of the sepals and petals, in centimeters. Based on these features, the goal is to predict which species of Iris the sample belongs to.\n",
        "\n",
        "\n",
        "For this exercise, you need to enable GPUs for this notebook:\n",
        "\n",
        "- Navigate to \"**Edit**\" → \"**Notebook Settings**\"\n",
        "- Select GPU from the **Hardware Accelerator** drop-down\n",
        "- You might need to rerun the notebook\n",
        "\n",
        "Next, we'll check if we can connect to the GPU with PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l23EV0A92F1J"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    print('Current device:', torch.cuda.get_device_name(device))\n",
        "else:\n",
        "    print('Failed to find GPU. Will use CPU.')\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d853a4f5816e49a6ac838230e0655087",
        "deepnote_cell_height": 62,
        "deepnote_cell_type": "markdown",
        "id": "lIB2XdFb2tgv",
        "tags": []
      },
      "source": [
        "###  Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "263a53c7e9754ea488149500a7842994",
        "deepnote_cell_height": 187,
        "deepnote_cell_type": "code",
        "id": "eMgXi4pF2tgv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "num_classes = 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEtvK8_Ilg8N"
      },
      "source": [
        "Process the data.\n",
        "\n",
        "**Question**: Is there anything we need to do with the default target? Why?\n",
        "\n",
        "**Answer**: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTJGr3RVltFT"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and test dataset\n",
        "# TODO\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# Process the data\n",
        "# TODO\n",
        "y_train_one_hot = ...\n",
        "y_test_one_hot = ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "379f8681cb0f4d54bdb2a83938685764",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "TaCmfbHE2tgw",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a9ab559f86de48bd8f2ee8f13a64fbda",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "rT-aJ-3f2tgw",
        "tags": []
      },
      "source": [
        "We will again use an MLP for this task (with Pytorch).\n",
        "\n",
        "Intitialize a model with **4 hidden layers** and a **hidden layer size of 768**.\n",
        "\n",
        "**Question**: Is there any else we should change when building the MLP to fit this task?\n",
        "\n",
        "**Hint**: it is no longer a binary classification problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QheJG1r0wp-s"
      },
      "source": [
        "**Answer:** ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuuDwsJ77J1l"
      },
      "outputs": [],
      "source": [
        "# Defining the model\n",
        "xor_mlp_pytorch = nn.Sequential(\n",
        "    ...\n",
        ")\n",
        "\n",
        "# Apply the initialization to the model\n",
        "xor_mlp_pytorch.apply(init_weights)\n",
        "\n",
        "# Defining loss function: Cross Entropy Loss\n",
        "loss_fn = ...\n",
        "\n",
        "# Defining optimizer: Stochastic gradient descent\n",
        "optimizer = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "651c8f0d9ea44ddc9ab7ad6aa7183498",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "lPRWF1BF2tgw",
        "is_collapsed": false,
        "tags": []
      },
      "source": [
        "### Training\n",
        "\n",
        "As you have learnt from the lecture, we can speed up the training process by **batching** and using **GPUs**. Modify the following code for batching and GPUs.\n",
        "\n",
        "You can also run the code before and after you make changes to see the speed up gain from batching and using GPU.\n",
        "\n",
        "**Hints**: You can make use of Pytorch's `DataLoader`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojseAt7t3Is1"
      },
      "source": [
        "**Question**: Report the execution time with and without GPU and batching.\n",
        "\n",
        "**Answer**: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOfkgN_p3WaA"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = xor_mlp_pytorch(X_train)\n",
        "    loss = loss_fn(output, y_train_one_hot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e383e8479b24456fb456a2b163651d8e",
        "deepnote_cell_height": 62,
        "deepnote_cell_type": "markdown",
        "id": "NGxuBpxx2tgw",
        "tags": []
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "12d63c77198848e08774450159eb4f5b",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "kRnjFbCD2tgx",
        "tags": []
      },
      "source": [
        "Show the overall accuracy of our model on the test dataset. Use the existing `accuracy` function that you implemented earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "f3491bbddfce45f3bbcac76872d99a7a",
        "deepnote_cell_height": 223,
        "deepnote_cell_type": "code",
        "id": "9ujmW5mT2tgx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "predictions = ...\n",
        "accuracy_value = ...\n",
        "print(accuracy_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "157c73719d994eb1a61b10d78034ae37",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "qK4VI1br2tgx",
        "tags": []
      },
      "source": [
        "Print the confusion matrix using `sklearn.metrics.confusion_matrix`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ce02ae5b924440b398b5ca899d94a6f6",
        "deepnote_cell_height": 97,
        "deepnote_cell_type": "code",
        "id": "8Tr40s4f2tgx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "cd41f0bd4689449b85001f94469b6dfb",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "9gtdQkiC2tgx",
        "tags": []
      },
      "source": [
        "**Question**: Now also look at the confusion matrix, what can you conclude from it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9bece33cdd404cd3bbf2c11a7ea557d5",
        "deepnote_cell_height": 52.390625,
        "deepnote_cell_type": "markdown",
        "id": "9gykcH6x2tgx",
        "tags": []
      },
      "source": [
        "**Answer**: ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40YpVK5KnxcZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "50c68a69-c321-4e24-b142-cdaa47048c60",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
