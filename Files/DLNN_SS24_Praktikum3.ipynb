{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G7Gs8hAJsog"
      },
      "source": [
        "# Meta-Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: the praktikums are for your own practice. They will **not be graded**!\n",
        "\n",
        "Remember to make a copy of this notebook to your own Colab. Changes made directly here will not be stored!"
      ],
      "metadata": {
        "id": "xFVchhXIE0iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenenver you see an ellipsis `...` and/or TODO comment, you're supposed to insert code or text answers."
      ],
      "metadata": {
        "id": "4lpEs28oFC6g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQW0zgZ2Jsoh"
      },
      "source": [
        "In this notebook, we will discuss three popular Meta-Learning algorithms: __Prototypical Networks__ ([Snell et al., 2017](https://arxiv.org/pdf/1703.05175.pdf)), __Model-Agnostic Meta-Learning / MAML__ ([Finn et al., 2017](http://proceedings.mlr.press/v70/finn17a.html)), and __Proto-MAML__ ([Triantafillou et al., 2020](https://openreview.net/pdf?id=rkgAGAVKPr)). We will focus on the implementation of __Proto-MAML__.\n",
        "\n",
        "We will focus on the task of few-shot classification where the training and test set have distinct sets of classes. For instance, we would train the model on the binary classifications of cats-birds and flowers-bikes, but during test time, the model would need to learn from 4 examples each the difference between dogs and otters, two classes we have not seen during training (Figure credit - [Lilian Weng](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/few-shot-classification.png?raw=1\" width=\"800px\"></center>\n",
        "\n",
        "\n",
        "\n",
        "First of all, let's start with importing our standard libraries. We will be using PyTorch Lightning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbzmO7ALJsoi"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from statistics import mean, stdev\n",
        "from copy import deepcopy\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100, SVHN\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Import tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial16\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkUMrhn2Jsoj"
      },
      "source": [
        "Training the models in this notebook can take between 2 and 8 hours, and the evaluation time of some algorithms is in the span of couples of minutes. Hence, we download pre-trained models and results below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUB_carwJsoj"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial16/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"ProtoNet.ckpt\", \"ProtoMAML.ckpt\",\n",
        "                    \"tensorboards/ProtoNet/events.out.tfevents.ProtoNet\",\n",
        "                    \"tensorboards/ProtoMAML/events.out.tfevents.ProtoMAML\",\n",
        "                    \"protomaml_fewshot.json\",\n",
        "                    \"protomaml_svhn_fewshot.json\"]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui2NI9jfJsoj"
      },
      "source": [
        "## Few-shot classification\n",
        "\n",
        "We start our implementation by discussing the dataset setup. In this notebook, we will use CIFAR100. CIFAR100 has 100 classes each with 600 images of size $32\\times 32$ pixels. Instead of splitting the training, validation, and test set over examples, we will split them over classes: we will use 80 classes for training, and 10 for validation, and 10 for testing. Our overall goal is to obtain a model that can distinguish between the 10 test classes with seeing very few examples. First, let's load the dataset and visualize some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xDNkavXJsoj"
      },
      "outputs": [],
      "source": [
        "# Loading CIFAR100 dataset\n",
        "CIFAR_train_set = CIFAR100(root=DATASET_PATH, train=True, download=True, transform=transforms.ToTensor())\n",
        "CIFAR_test_set = CIFAR100(root=DATASET_PATH, train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Oa6pvWQJsoj"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 12\n",
        "CIFAR_images = torch.stack([CIFAR_train_set[np.random.randint(len(CIFAR_train_set))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=6, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the CIFAR100 dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBG4sgYxJsok"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "Next, we need to prepare the dataset in the training, validation and test split as mentioned before. The torchvision package gives us the training and test set as two separate dataset objects. The next code cells will merge the original training and test set, and then create the new train-val-test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nGjVrQcJsok"
      },
      "outputs": [],
      "source": [
        "# Merging original training and test set\n",
        "CIFAR_all_images = np.concatenate([CIFAR_train_set.data, CIFAR_test_set.data], axis=0)\n",
        "CIFAR_all_targets = torch.LongTensor(CIFAR_train_set.targets + CIFAR_test_set.targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz-sPwqKJsok"
      },
      "source": [
        "To have an easier time handling the dataset, we define our own, simple dataset class below. It takes a set of images, labels/targets, and image transformations, and returns the corresponding images and labels element-wise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-u-hWzWJsok"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, imgs, targets, img_transform=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            imgs - Numpy array of shape [N,32,32,3] containing all images.\n",
        "            targets - PyTorch array of shape [N] containing all labels.\n",
        "            img_transform - A torchvision transformation that should be applied\n",
        "                            to the images before returning. If none, no transformation\n",
        "                            is applied.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_transform = img_transform\n",
        "        self.imgs = imgs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO\n",
        "        ...\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgs.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbtO09wqJsok"
      },
      "source": [
        "Now, we can create the class splits. We will assign the classes randomly to training, validation and test, and use a 80%-10%-10% split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5zhgUy6Jsok"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)           # Set seed for reproducibility\n",
        "\n",
        "# TODO\n",
        "train_classes, val_classes, test_classes = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLnkHZz-Jsok"
      },
      "source": [
        "To get an intuition of the validation and test classes, we print the class names below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifkyJb1IJsok"
      },
      "outputs": [],
      "source": [
        "# Printing validation and test classes\n",
        "idx_to_class = {val: key for key, val in CIFAR_train_set.class_to_idx.items()}\n",
        "print(\"Validation classes:\", [idx_to_class[c.item()] for c in val_classes])\n",
        "print(\"Test classes:\", [idx_to_class[c.item()] for c in test_classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_aGN_CEJsok"
      },
      "source": [
        "Remember that we want to learn the classification of the ten test classes from 80 other classes in our training set, and few examples from the actual test classes. We will experiment with the number of examples per class.\n",
        "\n",
        "Finally, we can create the training, validation and test dataset according to our split above. For this, we create dataset objects of our previously defined class `ImageDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glM63-kfJsok"
      },
      "outputs": [],
      "source": [
        "def dataset_from_labels(imgs, targets, class_set, **kwargs):\n",
        "    # TODO\n",
        "    class_mask = ...\n",
        "    return ImageDataset(imgs=imgs[class_mask],\n",
        "                        targets=targets[class_mask],\n",
        "                        **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U99V2vkJsok"
      },
      "source": [
        "We now normalize the dataset. Additionally, we use small augmentations during training to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUszcgkIJsok"
      },
      "outputs": [],
      "source": [
        "# Pre-computed statistics from the new train set\n",
        "DATA_MEANS = torch.Tensor([0.5183975 , 0.49192241, 0.44651328])\n",
        "DATA_STD = torch.Tensor([0.26770132, 0.25828985, 0.27961241])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(\n",
        "                                         DATA_MEANS, DATA_STD)\n",
        "                                     ])\n",
        "# For training, we add some augmentation.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop(\n",
        "                                          (32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(\n",
        "                                          DATA_MEANS, DATA_STD)\n",
        "                                      ])\n",
        "\n",
        "train_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, train_classes, img_transform=train_transform)\n",
        "val_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, val_classes, img_transform=test_transform)\n",
        "test_set = dataset_from_labels(\n",
        "    CIFAR_all_images, CIFAR_all_targets, test_classes, img_transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXcK-HNJsol"
      },
      "source": [
        "### Data sampling\n",
        "\n",
        "The strategy of how to use the available training data for learning few-shot adaptation is crucial in meta-learning. All three algorithms that we discuss here have a similar idea: simulate few-shot learning during training. Specifically, at each training step, we randomly select a small number of classes and sample a small number of examples for each class. This represents our few-shot training batch, which we also refer to as **support set**. Additionally, we sample a second set of examples from the same classes and refer to this batch as **query set**. Our training objective is to classify the query set correctly from seeing the support set and its corresponding labels. The main difference between our three methods (ProtoNet, MAML, and Proto-MAML) is in how they use the support set to adapt to the training classes.\n",
        "\n",
        "This subsection summarizes the code that is needed to create such training batches. In PyTorch, we can specify the data sampling procedure by so-called `Sampler` ([documentation](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)). Samplers are iterable objects that return indices in the order in which the data elements should be sampled. We usually used the option `shuffle=True` in the `data.DataLoader` objects which creates a sampler returning the data indices in random order. Here, we focus on samplers that return batches of indices that correspond to support and query set batches. Below, we implement such a sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YddSr_ErJsol"
      },
      "outputs": [],
      "source": [
        "class FewShotBatchSampler(object):\n",
        "\n",
        "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, shuffle_once=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            dataset_targets - PyTorch tensor of the labels of the data elements.\n",
        "            N_way - Number of classes to sample per batch.\n",
        "            K_shot - Number of examples to sample per class in the batch.\n",
        "            include_query - If True, returns batch of size N_way*K_shot*2, which\n",
        "                            can be split into support and query set. Simplifies\n",
        "                            the implementation of sampling the same classes but\n",
        "                            distinct examples for support and query set.\n",
        "            shuffle - If True, examples and classes are newly shuffled in each\n",
        "                      iteration (for training)\n",
        "            shuffle_once - If True, examples and classes are shuffled once in\n",
        "                           the beginning, but kept constant across iterations\n",
        "                           (for validation)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset_targets = dataset_targets\n",
        "        self.N_way = N_way\n",
        "        self.K_shot = K_shot\n",
        "        self.shuffle = shuffle\n",
        "        self.include_query = include_query\n",
        "        if self.include_query:\n",
        "            self.K_shot *= 2\n",
        "        self.batch_size = self.N_way * self.K_shot  # Number of overall images per batch\n",
        "\n",
        "        # Organize examples by class\n",
        "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.indices_per_class = {}\n",
        "        self.batches_per_class = {}  # Number of K-shot batches that each class can provide\n",
        "        for c in self.classes:\n",
        "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
        "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
        "\n",
        "        # Create a list of classes from which we select the N classes per batch\n",
        "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
        "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
        "        if shuffle_once or self.shuffle:\n",
        "            self.shuffle_data()\n",
        "        else:\n",
        "            # For testing, we iterate over classes instead of shuffling them\n",
        "            sort_idxs = [i+p*self.num_classes for i,\n",
        "                         c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
        "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
        "\n",
        "    def shuffle_data(self):\n",
        "        # Shuffle the examples per class\n",
        "        for c in self.classes:\n",
        "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
        "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
        "        # Shuffle the class list from which we sample. Note that this way of shuffling\n",
        "        # does not prevent to choose the same class twice in a batch. However, for\n",
        "        # training and validation, this is not a problem.\n",
        "        random.shuffle(self.class_list)\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Shuffle data\n",
        "        if self.shuffle:\n",
        "            self.shuffle_data()\n",
        "\n",
        "        # Sample few-shot batches\n",
        "        start_index = defaultdict(int)\n",
        "        for it in range(self.iterations):\n",
        "            # TODO: Select N classes for the batch\n",
        "            class_batch = ...\n",
        "            index_batch = []\n",
        "            # TODO: For each class, select the next K examples and add them to the batch\n",
        "            for c in class_batch:\n",
        "                index_batch.extend(...)\n",
        "                start_index[c] += self.K_shot\n",
        "\n",
        "            # If we return support+query set, sort them so that they are easy to split\n",
        "            if self.include_query:\n",
        "                index_batch = index_batch[::2] + index_batch[1::2]\n",
        "            yield index_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYs9gjoYJsol"
      },
      "source": [
        "Now, we can create our intended data loaders by passing an object of `FewShotBatchSampler` as `batch_sampler=...` input to the PyTorch data loader object. For our experiments, we will use a 5-class 4-shot training setting. This means that each support set contains 5 classes with 4 examples each, i.e., 20 images overall. Usually, it is good to keep the number of shots equal to the number that you aim to test on. However, we will experiment later with a different number of shots, and hence, we pick 4 as a compromise for now. To get the best-performing model, it is recommended to consider the number of training shots as hyperparameters in a grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6C_yaBlJsol"
      },
      "outputs": [],
      "source": [
        "N_WAY = 5\n",
        "K_SHOT = 4\n",
        "train_data_loader = data.DataLoader(train_set,\n",
        "                                    batch_sampler=FewShotBatchSampler(train_set.targets,\n",
        "                                                                      include_query=True,\n",
        "                                                                      N_way=N_WAY,\n",
        "                                                                      K_shot=K_SHOT,\n",
        "                                                                      shuffle=True),\n",
        "                                    num_workers=4)\n",
        "val_data_loader = data.DataLoader(val_set,\n",
        "                                  batch_sampler=FewShotBatchSampler(val_set.targets,\n",
        "                                                                    include_query=True,\n",
        "                                                                    N_way=N_WAY,\n",
        "                                                                    K_shot=K_SHOT,\n",
        "                                                                    shuffle=False,\n",
        "                                                                    shuffle_once=True),\n",
        "                                  num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_HA7ADAJsol"
      },
      "source": [
        "For simplicity, we implemented the sampling of a support and query set as sampling a support set with twice the number of examples. After sampling a batch from the data loader, we need to split it into a support and query set. We can summarize this step in the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuSOfYdUJsol"
      },
      "outputs": [],
      "source": [
        "def split_batch(imgs, targets):\n",
        "    support_imgs, query_imgs = imgs.chunk(2, dim=0)\n",
        "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
        "    return support_imgs, query_imgs, support_targets, query_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8tYlcKWJsol"
      },
      "source": [
        "Finally, to ensure that our implementation of the data sampling process is correct, we can sample a batch and visualize its support and query set. What we would like to see is that the support and query set have the same classes, but distinct examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXUmqosyJsol"
      },
      "outputs": [],
      "source": [
        "imgs, targets = next(iter(val_data_loader))  # We use the validation set since it does not apply augmentations\n",
        "support_imgs, query_imgs, _, _ = split_batch(imgs, targets)\n",
        "support_grid = torchvision.utils.make_grid(support_imgs, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
        "support_grid = support_grid.permute(1, 2, 0)\n",
        "query_grid = torchvision.utils.make_grid(query_imgs, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
        "query_grid = query_grid.permute(1, 2, 0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
        "ax[0].imshow(support_grid)\n",
        "ax[0].set_title(\"Support set\")\n",
        "ax[0].axis('off')\n",
        "ax[1].imshow(query_grid)\n",
        "ax[1].set_title(\"Query set\")\n",
        "ax[1].axis('off')\n",
        "plt.suptitle(\"Few Shot Batch\", weight='bold')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ooXzTC2Jsol"
      },
      "source": [
        "As we can see, the support and query set have the same five classes, but different examples. The models will be tasked to classify the examples in the query set by learning from the support set and its labels."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building blocks\n",
        "\n",
        "We define the encoder function $f_{\\theta}$ to be DenseNet:"
      ],
      "metadata": {
        "id": "JLzWeqdXjNLg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woJ4XhUcJsol"
      },
      "outputs": [],
      "source": [
        "def get_convnet(output_size):\n",
        "    convnet = torchvision.models.DenseNet(growth_rate=32,\n",
        "                                          block_config=(6, 6, 6, 6),  # 4 stages of 6 layers each\n",
        "                                          bn_size=2,  # bottleneck size is 2 times the growth rate\n",
        "                                          num_init_features=64,\n",
        "                                          num_classes=output_size  # Output dimensionality\n",
        "                                         )\n",
        "    return convnet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training function:"
      ],
      "metadata": {
        "id": "fuo5zee5jhqq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyUk2QEJJsom"
      },
      "outputs": [],
      "source": [
        "def train_model(model_class, train_loader, val_loader, **kwargs):\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, model_class.__name__),\n",
        "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "                         devices=1,\n",
        "                         max_epochs=200,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
        "                                    LearningRateMonitor(\"epoch\")],\n",
        "                         enable_progress_bar=False)\n",
        "    trainer.logger._default_hp_metric = None\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(\n",
        "        CHECKPOINT_PATH, model_class.__name__ + \".ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "        # Automatically loads the model with the saved hyperparameters\n",
        "        model = model_class.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)  # To be reproducable\n",
        "        model = model_class(**kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        model = model_class.load_from_checkpoint(\n",
        "            trainer.checkpoint_callback.best_model_path)  # Load best checkpoint after training\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fpn0aDiJsol"
      },
      "source": [
        "## Prototypical Networks\n",
        "\n",
        "- ProtoNet operates similarly to the nearest neighbor classification\n",
        "- It classifies a new example $\\mathbf{x}$ based on some distance function $d_{\\varphi}$ between $x$ and all elements in the support set.\n",
        "- First, ProtoNet uses an embedding function $f_{\\theta}$ to encode each input in the support set into a $L$-dimensional feature vector. Next, for each class $c$, we collect the feature vectors of all examples with label $c$ and average their feature vectors.\n",
        "- We take softmax over the distances of $\\mathbf{x}$ to all class prototypes.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/protonet_classification.svg?raw=1\" width=\"300px\"></center>\n",
        "\n",
        "\n",
        "- However, in this notebook, we will only use the idea of prototyping as a helper for the other methods. We will only implement the function to calculate the prototypes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWr0b-WKJsom"
      },
      "outputs": [],
      "source": [
        "class ProtoNet(pl.LightningModule):\n",
        "    @staticmethod\n",
        "    def calculate_prototypes(features, targets):\n",
        "        # Given a stack of features vectors and labels, return class prototypes\n",
        "        # features - shape [N, proto_dim], targets - shape [N]\n",
        "        classes, _ = torch.unique(targets).sort()  # Determine which classes we have\n",
        "        prototypes = []\n",
        "\n",
        "        for c in classes:\n",
        "            p = ...  # TODO: Average class feature vectors\n",
        "            prototypes.append(p)\n",
        "\n",
        "        prototypes = torch.stack(prototypes, dim=0)\n",
        "        # Return the 'classes' tensor to know which prototype belongs to which class\n",
        "        return prototypes, classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npca8VEaJson"
      },
      "source": [
        "## MAML and ProtoMAML\n",
        "\n",
        "- MAML, short for Model-Agnostic Meta-Learning, tries to adjust the standard optimization procedure to a few-shot setting.\n",
        "\n",
        "- Given a model, support, and query set during training, we optimize the model for $m$ steps on the support set and evaluate the gradients of the query loss with respect to the original model's parameters.\n",
        "\n",
        "- For the same model, we do it for a few different support-query sets and accumulate the gradients. This results in learning a model that provides a good initialization for being quickly adapted to the training tasks. If we denote the model parameters with $\\theta$, we can visualize the procedure as follows (Figure credit - [Finn et al.](http://proceedings.mlr.press/v70/finn17a.html)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/MAML_figure.svg?raw=1\" width=\"300px\"></center>\n",
        "\n",
        "The full algorithm of MAML is as follows:\n",
        "-  At each training step, we sample a batch of tasks, i.e., a batch of support-query set pairs.\n",
        "- For each task $T_i$, we optimize a model $f_{\\theta}$ on the support set via SGD, and denote this model as $f_{\\theta_i'}$. We refer to this optimization as _inner loop_. Using this new model, we calculate the gradients of the original parameters, $\\theta$, with respect to the query loss on $f_{\\theta_i'}$.\n",
        "- These gradients are accumulated over all tasks and used to update $\\theta$. This is called _outer loop_ since we iterate over tasks. The full MAML algorithm is summarized below (Figure credit - [Finn et al.](http://proceedings.mlr.press/v70/finn17a.html)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial16/MAML_algorithm.svg?raw=1\" width=\"400px\"></center>\n",
        "\n",
        "\n",
        "To obtain gradients for the initial parameters $\\theta$ from the optimized model $f_{\\theta_i'}$, we actually need second-order gradients, i.e. gradients of gradients, as the support set gradients depend on $\\theta$ as well.\n",
        "\n",
        "\n",
        "A simpler, yet almost equally well-performing alternative is First-Order MAML (FOMAML): calculate the outer loop gradients (line 10 in algorithm 2) simply by calculating the gradients with respect to $\\theta_i'$ and use those as an update to $\\theta$. Hence, the new update rule becomes:\n",
        "\n",
        "$$\n",
        "\\theta\\leftarrow\\theta-\\beta\\sum_{T_i\\sim p(T)}\\nabla_{\\theta_i'}L_{T_i}(f_{\\theta_i'})\n",
        "$$\n",
        "\n",
        "Note the change of $\\theta$ to $\\theta_i'$ for $\\nabla$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFXSC0rQJson"
      },
      "source": [
        "### ProtoMAML\n",
        "\n",
        "- A problem of MAML is how to design the output classification layer. In case all tasks have a different number of classes, we need to initialize the output layer with zeros or randomly in every iteration.\n",
        "Even if we always have the same number of classes, we just start from random predictions.\n",
        "\n",
        "- To overcome this problem, Triantafillou et al. (2020) propose to use prototypes to initialize our output layer to have a strong initialization.\n",
        "\n",
        "- It can be shown that the softmax over euclidean distances can be reformulated as a linear layer with softmax. If we initialize the output weight with twice the prototypes, and the biases by the negative squared L2 norm of the prototypes, we start with a Prototypical Network.\n",
        "\n",
        "- In the following, we will implement First-Order ProtoMAML for few-shot classification. The implementation of MAML would be the same except for the output layer initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb0_5isqJson"
      },
      "source": [
        "### ProtoMAML implementation\n",
        "\n",
        "- At each training step, we first sample a batch of tasks, and a support and query set for each task. In our case of few-shot classification, this means that we simply sample multiple support-query set pairs from our sampler.\n",
        "\n",
        "- For each task, we finetune our current model on the support set. However, since we need to remember the original parameters for the other tasks, the outer loop gradient update, and future training steps, we need to create a copy of our model and finetune only the copy. We can copy a model by using standard Python functions like `deepcopy`. The inner loop is implemented in the function `adapt_few_shot` in the PyTorch Lightning module below.\n",
        "\n",
        "- After finetuning the model, we apply it to the query set and calculate the first-order gradients with respect to the original parameters $\\theta$.\n",
        "\n",
        "- After calculating all gradients and summing them together in the original model, we can take a standard optimizer step.\n",
        "\n",
        "For simplicity, we stick with first-order methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRZ2CrsDJsoo"
      },
      "outputs": [],
      "source": [
        "class ProtoMAML(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, proto_dim, lr, lr_inner, lr_output, num_inner_steps):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            proto_dim - Dimensionality of prototype feature space\n",
        "            lr - Learning rate of the outer loop Adam optimizer\n",
        "            lr_inner - Learning rate of the inner loop SGD optimizer\n",
        "            lr_output - Learning rate for the output layer in the inner loop\n",
        "            num_inner_steps - Number of inner loop updates to perform\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = get_convnet(output_size=self.hparams.proto_dim)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[140,180], gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def run_model(self, local_model, output_weight, output_bias, imgs, labels):\n",
        "        # Execute a model with given output layer weights and inputs\n",
        "        feats = local_model(imgs)\n",
        "        preds = F.linear(feats, output_weight, output_bias)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=1) == labels).float()\n",
        "        return loss, preds, acc\n",
        "\n",
        "    def adapt_few_shot(self, support_imgs, support_targets):\n",
        "        # Determine prototype initialization\n",
        "        support_feats = self.model(support_imgs)\n",
        "        prototypes, classes = ProtoNet.calculate_prototypes(support_feats, support_targets)\n",
        "        support_labels = (classes[None,:] == support_targets[:,None]).long().argmax(dim=-1)\n",
        "        # TODO: Create inner-loop model and optimizer\n",
        "\n",
        "        local_model = ...\n",
        "\n",
        "        # TODO: Create output layer weights with prototype-based initialization\n",
        "        init_weight = ...\n",
        "        init_bias = ...\n",
        "        output_weight = init_weight.detach().requires_grad_()\n",
        "        output_bias = init_bias.detach().requires_grad_()\n",
        "\n",
        "        # Optimize inner loop model on support set\n",
        "        for _ in range(self.hparams.num_inner_steps):\n",
        "            # Determine loss on the support set\n",
        "            loss, _, _ = self.run_model(local_model, output_weight, output_bias, support_imgs, support_labels)\n",
        "            # Calculate gradients and perform inner loop update\n",
        "            loss.backward()\n",
        "            local_optim.step()\n",
        "            # Update output layer via SGD\n",
        "            # (https://discuss.pytorch.org/t/the-difference-between-torch-tensor-data-and-torch-tensor/25995/4):\n",
        "            with torch.no_grad():\n",
        "                output_weight.copy_(output_weight - self.hparams.lr_output * output_weight.grad)\n",
        "                output_bias.copy_(output_bias - self.hparams.lr_output * output_bias.grad)\n",
        "\n",
        "            # Reset gradients\n",
        "            local_optim.zero_grad()\n",
        "            output_weight.grad.fill_(0)\n",
        "            output_bias.grad.fill_(0)\n",
        "\n",
        "        # Re-attach computation graph of prototypes\n",
        "        output_weight = (output_weight - init_weight).detach() + init_weight\n",
        "        output_bias = (output_bias - init_bias).detach() + init_bias\n",
        "\n",
        "        return local_model, output_weight, output_bias, classes\n",
        "\n",
        "    def outer_loop(self, batch, mode=\"train\"):\n",
        "        accuracies = []\n",
        "        losses = []\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Determine gradients for batch of tasks\n",
        "        for task_batch in batch:\n",
        "            imgs, targets = task_batch\n",
        "            support_imgs, query_imgs, support_targets, query_targets = split_batch(imgs, targets)\n",
        "\n",
        "            # Perform inner loop adaptation\n",
        "            local_model, output_weight, output_bias, classes = ...\n",
        "            # Determine loss of query set\n",
        "            query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
        "            loss, preds, acc = self.run_model(local_model, output_weight, output_bias, query_imgs, query_labels)\n",
        "\n",
        "            # Calculate gradients for query set loss\n",
        "            if mode == \"train\":\n",
        "                loss.backward()\n",
        "\n",
        "                for p_global, p_local in zip(self.model.parameters(), local_model.parameters()):\n",
        "                    # First-order approx. -> add gradients of finetuned and base model\n",
        "                    ...\n",
        "\n",
        "            accuracies.append(acc.mean().detach())\n",
        "            losses.append(loss.detach())\n",
        "\n",
        "        # Perform update of base model\n",
        "        if mode == \"train\":\n",
        "            opt = self.optimizers()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        self.log(f\"{mode}_loss\", sum(losses) / len(losses))\n",
        "        self.log(f\"{mode}_acc\", sum(accuracies) / len(accuracies))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.outer_loop(batch, mode=\"train\")\n",
        "        return None  # Returning None means we skip the default training optimizer steps by PyTorch Lightning\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # Validation requires to finetune a model, hence we need to enable gradients\n",
        "        torch.set_grad_enabled(True)\n",
        "        self.outer_loop(batch, mode=\"val\")\n",
        "        torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haA1mc-uJsoo"
      },
      "source": [
        "### Training\n",
        "\n",
        "To train ProtoMAML, we need to sample multiple support-query set batches. To implement this, we use another Sampler that combines multiple batches from a `FewShotBatchSampler` and returns it afterward. Additionally, we define a `collate_fn` for our data loader which takes the stack of support-query set images and returns the tasks as a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGJbPvlJJsoo"
      },
      "outputs": [],
      "source": [
        "class TaskBatchSampler(object):\n",
        "\n",
        "    def __init__(self, dataset_targets, batch_size, N_way, K_shot, include_query=False, shuffle=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            dataset_targets - PyTorch tensor of the labels of the data elements.\n",
        "            batch_size - Number of tasks to aggregate in a batch\n",
        "            N_way - Number of classes to sample per batch.\n",
        "            K_shot - Number of examples to sample per class in the batch.\n",
        "            include_query - If True, returns batch of size N_way*K_shot*2, which\n",
        "                            can be split into support and query set. Simplifies\n",
        "                            the implementation of sampling the same classes but\n",
        "                            distinct examples for support and query set.\n",
        "            shuffle - If True, examples and classes are newly shuffled in each\n",
        "                      iteration (for training)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.batch_sampler = FewShotBatchSampler(dataset_targets, N_way, K_shot, include_query, shuffle)\n",
        "        self.task_batch_size = batch_size\n",
        "        self.local_batch_size = self.batch_sampler.batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Aggregate multiple batches before returning the indices\n",
        "        batch_list = []\n",
        "        for batch_idx, batch in enumerate(self.batch_sampler):\n",
        "            batch_list.extend(batch)\n",
        "            if (batch_idx+1) % self.task_batch_size == 0:\n",
        "                yield batch_list\n",
        "                batch_list = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)//self.task_batch_size\n",
        "\n",
        "    def get_collate_fn(self):\n",
        "        # Returns a collate function that converts one big tensor into a list of task-specific tensors\n",
        "        def collate_fn(item_list):\n",
        "            imgs = torch.stack([img for img, target in item_list], dim=0)\n",
        "            targets = torch.stack([target for img, target in item_list], dim=0)\n",
        "            imgs = imgs.chunk(self.task_batch_size, dim=0)\n",
        "            targets = targets.chunk(self.task_batch_size, dim=0)\n",
        "            return list(zip(imgs, targets))\n",
        "        return collate_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9pbf9_gJsoo"
      },
      "source": [
        "Create the data loaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMEcMdprJsoo"
      },
      "outputs": [],
      "source": [
        "# Training constant\n",
        "N_WAY = 5\n",
        "K_SHOT = 4\n",
        "\n",
        "# Training set\n",
        "train_protomaml_sampler = TaskBatchSampler(train_set.targets,\n",
        "                                           include_query=True,\n",
        "                                           N_way=N_WAY,\n",
        "                                           K_shot=K_SHOT,\n",
        "                                           batch_size=16)\n",
        "train_protomaml_loader = data.DataLoader(train_set,\n",
        "                                         batch_sampler=train_protomaml_sampler,\n",
        "                                         collate_fn=train_protomaml_sampler.get_collate_fn(),\n",
        "                                         num_workers=2)\n",
        "\n",
        "# Validation set\n",
        "val_protomaml_sampler = TaskBatchSampler(val_set.targets,\n",
        "                                         include_query=True,\n",
        "                                         N_way=N_WAY,\n",
        "                                         K_shot=K_SHOT,\n",
        "                                         batch_size=1,  # We do not update the parameters, hence the batch size is irrelevant here\n",
        "                                         shuffle=False)\n",
        "val_protomaml_loader = data.DataLoader(val_set,\n",
        "                                       batch_sampler=val_protomaml_sampler,\n",
        "                                       collate_fn=val_protomaml_sampler.get_collate_fn(),\n",
        "                                       num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tvLYICQJsoo"
      },
      "source": [
        "Now, we are ready to train our ProtoMAML:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ssyegUtsJsoo"
      },
      "outputs": [],
      "source": [
        "protomaml_model = train_model(ProtoMAML,\n",
        "                              proto_dim=64,\n",
        "                              lr=1e-3,\n",
        "                              lr_inner=0.1,\n",
        "                              lr_output=0.1,\n",
        "                              num_inner_steps=1,  # Often values between 1 and 10\n",
        "                              train_loader=train_protomaml_loader,\n",
        "                              val_loader=val_protomaml_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR8c03KzJsoo"
      },
      "source": [
        "### Testing\n",
        "\n",
        "We test ProtoMAML by picking random examples in the test set as support sets and use the rest of the dataset as the query set. We need to finetune a separate model for each support set. This is why this process is expensive, and in our case, testing $k=\\{2,4,8,16,32\\}$ can take almost an hour. Hence, we provide evaluation files besides the pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIi3OkYMJsoo"
      },
      "outputs": [],
      "source": [
        "def test_protomaml(model, dataset, k_shot=4):\n",
        "    pl.seed_everything(42)\n",
        "    model = model.to(device)\n",
        "    num_classes = dataset.targets.unique().shape[0]\n",
        "    exmps_per_class = dataset.targets.shape[0]//num_classes\n",
        "\n",
        "    # Data loader for full test set as query set\n",
        "    full_dataloader = data.DataLoader(dataset,\n",
        "                                      batch_size=128,\n",
        "                                      num_workers=4,\n",
        "                                      shuffle=False,\n",
        "                                      drop_last=False)\n",
        "    # Data loader for sampling support sets\n",
        "    sampler = FewShotBatchSampler(dataset.targets,\n",
        "                                  include_query=False,\n",
        "                                  N_way=num_classes,\n",
        "                                  K_shot=k_shot,\n",
        "                                  shuffle=False,\n",
        "                                  shuffle_once=False)\n",
        "    sample_dataloader = data.DataLoader(dataset,\n",
        "                                        batch_sampler=sampler,\n",
        "                                        num_workers=2)\n",
        "\n",
        "    # We iterate through the full dataset in two manners. First, to select the k-shot batch.\n",
        "    # Second, the evaluate the model on all other examples\n",
        "    accuracies = []\n",
        "    for (support_imgs, support_targets), support_indices in tqdm(zip(sample_dataloader, sampler), \"Performing few-shot finetuning\"):\n",
        "        support_imgs = support_imgs.to(device)\n",
        "        support_targets = support_targets.to(device)\n",
        "        # TODO: Finetune new model on support set\n",
        "        local_model, output_weight, output_bias, classes = ...\n",
        "        with torch.no_grad():  # No gradients for query set needed\n",
        "            local_model.eval()\n",
        "            batch_acc = torch.zeros((0,), dtype=torch.float32, device=device)\n",
        "            # Evaluate all examples in test dataset\n",
        "            for query_imgs, query_targets in full_dataloader:\n",
        "                query_imgs = query_imgs.to(device)\n",
        "                query_targets = query_targets.to(device)\n",
        "                query_labels = (classes[None,:] == query_targets[:,None]).long().argmax(dim=-1)\n",
        "                _, _, acc = ...\n",
        "                batch_acc = torch.cat([batch_acc, acc.detach()], dim=0)\n",
        "            # Exclude support set elements\n",
        "            for s_idx in support_indices:\n",
        "                batch_acc[s_idx] = 0\n",
        "            batch_acc = batch_acc.sum().item() / (batch_acc.shape[0] - len(support_indices))\n",
        "            accuracies.append(batch_acc)\n",
        "    return mean(accuracies), stdev(accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUngVOIaJsop"
      },
      "source": [
        "In contrast to training, it is recommended to use many more inner loop updates during testing. During training, we are not interested in getting the best model from the inner loop, but the model which can provide the best gradients. Hence, one update might be already sufficient in training, but for testing, it was often observed that a larger number of updates can give a considerable performance boost. Thus, we change the inner loop updates to 200 before testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtIhC1UpJsop"
      },
      "outputs": [],
      "source": [
        "protomaml_model.hparams.num_inner_steps = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg4wCAVzJsop"
      },
      "source": [
        "Now, we can test our model. For the pre-trained models, we provide a json file with the results to reduce evaluation time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZtAjNVIJsop"
      },
      "outputs": [],
      "source": [
        "protomaml_result_file = os.path.join(CHECKPOINT_PATH, \"protomaml_fewshot.json\")\n",
        "\n",
        "if os.path.isfile(protomaml_result_file):\n",
        "    # Load pre-computed results\n",
        "    with open(protomaml_result_file, 'r') as f:\n",
        "        protomaml_accuracies = json.load(f)\n",
        "    protomaml_accuracies = {int(k): v for k, v in protomaml_accuracies.items()}\n",
        "else:\n",
        "    # Perform same experiments as for ProtoNet\n",
        "    protomaml_accuracies = dict()\n",
        "    for k in [2, 4, 8, 16, 32]:\n",
        "        protomaml_accuracies[k] = test_protomaml(protomaml_model, test_set, k_shot=k)\n",
        "    # Export results\n",
        "    with open(protomaml_result_file, 'w') as f:\n",
        "        json.dump(protomaml_accuracies, f, indent=4)\n",
        "\n",
        "for k in protomaml_accuracies:\n",
        "    print(f\"Accuracy for k={k}: {100.0*protomaml_accuracies[k][0]:4.2f}% (+-{100.0*protomaml_accuracies[k][1]:4.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inDob0RnJsop"
      },
      "source": [
        "Plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_few_shot(acc_dict, name, color=None, ax=None):\n",
        "    sns.set()\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1,1,figsize=(5,3))\n",
        "    ks = sorted(list(acc_dict.keys()))\n",
        "    mean_accs = [acc_dict[k][0] for k in ks]\n",
        "    std_accs = [acc_dict[k][1] for k in ks]\n",
        "    ax.plot(ks, mean_accs, marker='o', markeredgecolor='k', markersize=6, label=name, color=color)\n",
        "    ax.fill_between(ks, [m-s for m,s in zip(mean_accs, std_accs)], [m+s for m,s in zip(mean_accs, std_accs)], alpha=0.2, color=color)\n",
        "    ax.set_xticks(ks)\n",
        "    ax.set_xlim([ks[0]-1, ks[-1]+1])\n",
        "    ax.set_xlabel(\"Number of shots per class\", weight='bold')\n",
        "    ax.set_ylabel(\"Accuracy\", weight='bold')\n",
        "    if len(ax.get_title()) == 0:\n",
        "        ax.set_title(\"Few-Shot Performance \" + name, weight='bold')\n",
        "    else:\n",
        "        ax.set_title(ax.get_title() + \" and \" + name, weight='bold')\n",
        "    ax.legend()\n",
        "    return ax"
      ],
      "metadata": {
        "id": "8Imr4fe3iban"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSn_AzHxJsop"
      },
      "outputs": [],
      "source": [
        "plot_few_shot(protomaml_accuracies, name=\"ProtoMAML\", color=\"C2\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArT9aTylKIDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}