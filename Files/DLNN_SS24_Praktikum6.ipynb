{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmSNTXp3huEk"
      },
      "source": [
        "# Praktikum 6 - Adversarial attacks\n",
        "\n",
        "Note: the praktikums are for your own practice. They will **not be graded**!\n",
        "\n",
        "Remember to make a copy of this notebook to your own Colab. Changes made directly here will not be stored!\n",
        "\n",
        "Whenenver you see an ellipsis `...` and/or TODO comment, you're supposed to insert code or text answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR_-WtYJhuEl"
      },
      "source": [
        "In this praktikum, we will discuss adversarial attacks on deep image classification models.\n",
        "\n",
        "Let's being with importing our standard libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2pLAV_yhuEl"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial10\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11hvtwQohuEm"
      },
      "source": [
        "We have again a few download statements. This includes both a dataset, and a few pretrained patches we will use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYYcUcrshuEm"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import zipfile\n",
        "# Github URL where the dataset is stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "# Files to download\n",
        "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\"), (CHECKPOINT_PATH, \"patches.zip\")]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for dir_name, file_name in pretrained_files:\n",
        "    file_path = os.path.join(dir_name, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
        "        if file_name.endswith(\".zip\"):\n",
        "            print(\"Unzipping file...\")\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4ashNg2huEm"
      },
      "source": [
        "## Deep CNNs on ImageNet\n",
        "\n",
        "For our experiments in this notebook, we will use a common CNN architecture (ResNet34) trained on the ImageNet dataset, which we can load from the torchvision package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6ot2ILHhuEm"
      },
      "outputs": [],
      "source": [
        "# Load CNN architecture pretrained on ImageNet\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# No gradients needed for the network\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MYXhtwhuEm"
      },
      "source": [
        "To perform adversarial attacks, we also need a dataset to work on. Given that the CNN model has been trained on ImageNet, we will perform the attacks on data from ImageNet.\n",
        "\n",
        "For this, we provide a small set of pre-processed images from the original ImageNet dataset, where we have 5 images for each of the 1000 labels of the dataset. We can load the data below, and create a corresponding data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arKtYKsEhuEm"
      },
      "outputs": [],
      "source": [
        "# Mean and Std from ImageNet\n",
        "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
        "# No resizing and center crop necessary as images are already preprocessed.\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN,\n",
        "                         std=NORM_STD)\n",
        "])\n",
        "\n",
        "# Load dataset and create data loader\n",
        "imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
        "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
        "                                     f\"Please make sure to have downloaded the ImageNet dataset here, or change the {DATASET_PATH=} variable.\"\n",
        "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=False, drop_last=False, num_workers=8)\n",
        "\n",
        "# Load label names to interpret the label numbers 0 to 999\n",
        "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
        "    label_names = json.load(f)\n",
        "\n",
        "def get_label_index(lab_str):\n",
        "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
        "    return label_names.index(lab_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLTuVkOHhuEm"
      },
      "source": [
        "Before we start with our attacks, we should verify the performance of our model. We will use the \"Top-5 accuracy\" metric, which tells us how many times the true label has been within the 5 most-likely predictions.\n",
        "\n",
        "As models usually perform quite well on those, we report the error (1 - accuracy) instead of the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5XaoTfshuEm"
      },
      "outputs": [],
      "source": [
        "def eval_model(dataset_loader, img_func=None):\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    for imgs, labels in tqdm(dataset_loader, desc=\"Validating...\"):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        if img_func is not None:\n",
        "            imgs = img_func(imgs, labels)\n",
        "        with torch.no_grad():\n",
        "            preds = pretrained_model(imgs)\n",
        "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
        "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        counter += preds.shape[0]\n",
        "    acc = tp.float().item()/counter\n",
        "    top5 = tp_5.float().item()/counter\n",
        "    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n",
        "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWSAw-OthuEm"
      },
      "outputs": [],
      "source": [
        "_ = eval_model(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaTKRm3xhuEm"
      },
      "source": [
        "The ResNet34 achives a decent error rate of 4.3% for the top-5 predictions. Next, we can look at some predictions of the model to get more familiar with the dataset. The function below plots an image along with a bar diagram of its predictions. We also prepare it to show adversarial examples for later applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKSYSVvchuEm"
      },
      "outputs": [],
      "source": [
        "def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        # Tensor image to numpy\n",
        "        img = img.cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "        label = label.item()\n",
        "\n",
        "    # Plot on the left the image with the true label as title.\n",
        "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
        "    if noise is None or adv_img is None:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
        "\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title(label_names[label])\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    if adv_img is not None and noise is not None:\n",
        "        # Visualize adversarial images\n",
        "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
        "        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
        "        ax[1].imshow(adv_img)\n",
        "        ax[1].set_title('Adversarial')\n",
        "        ax[1].axis('off')\n",
        "        # Visualize noise\n",
        "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
        "        noise = noise * 0.5 + 0.5 # Scale between 0 to 1\n",
        "        ax[2].imshow(noise)\n",
        "        ax[2].set_title('Noise')\n",
        "        ax[2].axis('off')\n",
        "        # buffer\n",
        "        ax[3].axis('off')\n",
        "\n",
        "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
        "        pred = torch.softmax(pred, dim=-1)\n",
        "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
        "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
        "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
        "    ax[-1].set_yticks(np.arange(K))\n",
        "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
        "    ax[-1].invert_yaxis()\n",
        "    ax[-1].set_xlabel('Confidence')\n",
        "    ax[-1].set_title('Predictions')\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-uonFmkhuEm"
      },
      "source": [
        "Let's visualize a few images below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg_4nCZohuEm"
      },
      "outputs": [],
      "source": [
        "exmp_batch, label_batch = next(iter(data_loader))\n",
        "with torch.no_grad():\n",
        "    preds = pretrained_model(exmp_batch.to(device))\n",
        "for i in range(1,17,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js9ZAzKMhuEn"
      },
      "source": [
        "The bar plot on the right shows the top-5 predictions of the model with their class probabilities. We denote the class probabilities with \"confidence\" as it somewhat resembles how confident the network is that the image is of one specific class.\n",
        "\n",
        "Some of the images have a highly peaked probability distribution, and we would expect the model to be rather robust against noise for those. However, we will see below that this is not always the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCHA1YDEhuEn"
      },
      "source": [
        "## White-box adversarial attacks\n",
        "\n",
        "Adversarial attacks are usually grouped into \"white-box\" and \"black-box\" attacks. White-box attacks assume that we have access to the model parameter. Black-box attacks on the other hand have the harder task of not having any knowledge about the network, and can only obtain predictions for an image.\n",
        "\n",
        "In this notebook, we will focus on white-box attacks as they are usually easier to implement.\n",
        "\n",
        "### Fast Gradient Sign Method (FGSM)\n",
        "\n",
        "One of the first attack strategies proposed is Fast Gradient Sign Method (FGSM). Given an image, we create an adversarial example by the following expression:\n",
        "\n",
        "$$\\tilde{x} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta,x,y))$$\n",
        "\n",
        "The term $J(\\theta,x,y)$ represents the loss of the network for classifying input image $x$ as label $y$; $\\epsilon$ is the intensity of the noise, and $\\tilde{x}$ the final adversarial example. The equation resembles SGD and is actually nothing else than that. We change the input image $x$ in the direction of *maximizing* the loss $J(\\theta,x,y)$. This is exactly the other way round as during training, where we try to minimize the loss.\n",
        "\n",
        "The sign function and $\\epsilon$ can be seen as gradient clipping and learning rate specifically. We only allow our attack to change each pixel value by $\\epsilon$. You can also see that the attack can be performed very fast, as it only requires a single forward and backward pass. Let's implement it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUKWWk2xhuEn"
      },
      "outputs": [],
      "source": [
        "def fast_gradient_sign_method(model, imgs, labels, epsilon=0.02):\n",
        "    # Determine prediction of the model\n",
        "    # Clone the input image and make it require gradients, since we will need to update the input\n",
        "    inp_imgs = ...\n",
        "    # Pass the input through the model to get the logits\n",
        "    preds = ...\n",
        "    # Pass the logits through log softmax\n",
        "    preds = ...\n",
        "\n",
        "    # Calculate loss by NLL, and do a backward pass\n",
        "    loss = ...\n",
        "    loss.sum().backward()\n",
        "\n",
        "    # Update image to adversarial example as written above\n",
        "    noise_grad = ...\n",
        "    fake_imgs = ...\n",
        "    fake_imgs.detach_()\n",
        "    return fake_imgs, noise_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k1Dx_ijhuEn"
      },
      "source": [
        "The default value of $\\epsilon=0.02$ corresponds to changing a pixel value by about 1 in the range of 0 to 255, e.g. changing 127 to 128. This difference is marginal and can often not be recognized by humans. Let's try it below on our example images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ywsDZLS9huEn"
      },
      "outputs": [],
      "source": [
        "adv_imgs, noise_grad = fast_gradient_sign_method(pretrained_model, exmp_batch, label_batch, epsilon=0.02)\n",
        "with torch.no_grad():\n",
        "    adv_preds = pretrained_model(adv_imgs.to(device))\n",
        "\n",
        "for i in range(1,17,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds[i], adv_img=adv_imgs[i], noise=noise_grad[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3UhKu2DhuEn"
      },
      "source": [
        "**Question**: Did we succeeded on fooling the network on these examples?\n",
        "\n",
        "**Answer**: ...\n",
        "\n",
        "\n",
        "We can also check the accuracy of the model on the adversarial images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovVFO6mrhuEn"
      },
      "outputs": [],
      "source": [
        "_ = eval_model(data_loader, img_func=lambda x, y: fast_gradient_sign_method(pretrained_model, x, y, epsilon=0.02)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3iEMtmNhuEn"
      },
      "source": [
        "**Question**: How is the performance compared to the error rate of 4.3% on the clean images? Do you have any comment on the wrongly-outputed labels on the examples?\n",
        "\n",
        "**Answer**: ...\n",
        "\n",
        "\n",
        "FGSM could be adapted to increase the probability of a specific class instead of minimizing the probability of a label, but for those, there are usually better attacks such as the adversarial patch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9EN84iwhuEn"
      },
      "source": [
        "### Adversarial Patches\n",
        "\n",
        "Instead of changing every pixel by a little bit, we can also try to change a small part of the image into whatever values we would like. In other words, we will create a small image patch that covers a minor part of the original image but causes the model to confidentially predict a specific class we choose.\n",
        "\n",
        "\n",
        "We train such patches by calculating gradients for the input, and update our adversarial input correspondingly. However, different from FGSM, we do not calculate a gradient for every pixel. Instead, we replace parts of the input image with our patch and then calculate the gradients just for our patch. Secondly, we don't just do it for one image, but we want the patch to work with any possible image.\n",
        "\n",
        "Hence, we have a whole training loop where we train the patch using SGD. Lastly, image patches are usually designed to make the model predict a specific class, not just any other arbitrary class except the true label. For instance, we can try to create a patch for the class \"toaster\" and train the patch so that our pretrained model predicts the class \"toaster\" for any image with the patch in it.\n",
        "\n",
        "Given a batch of input images and a patch, we can add the patch as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdZSYiPThuEn"
      },
      "outputs": [],
      "source": [
        "def place_patch(img, patch):\n",
        "    for i in range(img.shape[0]):\n",
        "        # Put the patch in a random location in the image\n",
        "        h_offset = ...\n",
        "        w_offset = ...\n",
        "        ...\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWMTtBQNhuEn"
      },
      "source": [
        "The patch itself will be an `nn.Parameter` whose values are in the range between $-\\infty$ and $\\infty$. Images are, however, naturally limited in their range, and thus we write a small function that maps the parameter into the image value range of ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGjgwxJVhuEn"
      },
      "outputs": [],
      "source": [
        "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
        "def patch_forward(patch):\n",
        "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
        "    patch = ...\n",
        "    return patch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez0VUI3EhuEn"
      },
      "source": [
        "Before looking at the actual training code, we can write a small evaluation function. We evaluate the success of a patch by how many times we were able to fool the network into predicting our target class. A simple function for this is implemented below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khXdYP4_huEn"
      },
      "outputs": [],
      "source": [
        "def eval_patch(model, patch, val_loader, target_class):\n",
        "    model.eval()\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
        "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
        "            for _ in range(4):\n",
        "                patch_img = place_patch(img, patch)\n",
        "                patch_img = patch_img.to(device)\n",
        "                img_labels = img_labels.to(device)\n",
        "                pred = model(patch_img)\n",
        "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
        "                # as we would not \"fool\" the model into predicting those\n",
        "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
        "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
        "                counter += (img_labels != target_class).sum()\n",
        "    acc = tp/counter\n",
        "    top5 = tp_5/counter\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJzthvtqhuEn"
      },
      "source": [
        "Finally, we can look at the training loop. Given a model to fool, a target class to design the patch for, and a size $k$ of the patch in the number of pixels, we first start by creating a parameter of size $3\\times k\\times k$. These are the only parameters we will train, and the network itself remains untouched.\n",
        "\n",
        "We use a simple SGD optimizer with momentum to minimize the classification loss of the model given the patch in the image. In the end, the patch will represent patterns that are characteristic of the class. Over the iterations, the model finetunes the pattern and, hopefully, achieves a high fooling accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6rzwU0KhuEn"
      },
      "outputs": [],
      "source": [
        "def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n",
        "    # Leave a small set of images out to check generalization\n",
        "    # In most of our experiments, the performance on the hold-out data points\n",
        "    # was as good as on the training set. Overfitting was little possible due\n",
        "    # to the small size of the patches.\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "    # Create parameter and optimizer\n",
        "    if not isinstance(patch_size, tuple):\n",
        "        patch_size = (patch_size, patch_size)\n",
        "\n",
        "    patch = nn.Parameter(...)\n",
        "\n",
        "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        t = tqdm(train_loader, leave=False)\n",
        "        for img, _ in t:\n",
        "            img = ...\n",
        "            pred = ...\n",
        "            labels = ...\n",
        "            loss = ...\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
        "\n",
        "    # Final validation\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "\n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp7Sj5TYhuEn"
      },
      "source": [
        "To get some experience with what to expect from an adversarial patch attack, we want to train multiple patches for different classes. As the training of a patch can take one or two minutes on a GPU, we have provided a couple of pre-trained patches including their results on the full dataset. The results are saved in a JSON file, which is loaded below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4jfQayehuEn"
      },
      "outputs": [],
      "source": [
        "# Load evaluation results of the pretrained patches\n",
        "json_results_file = os.path.join(CHECKPOINT_PATH, \"patch_results.json\")\n",
        "json_results = {}\n",
        "if os.path.isfile(json_results_file):\n",
        "    with open(json_results_file, \"r\") as f:\n",
        "        json_results = json.load(f)\n",
        "\n",
        "# If you train new patches, you can save the results via calling this function\n",
        "def save_results(patch_dict):\n",
        "    result_dict = {cname: {psize: [t.item() if isinstance(t, torch.Tensor) else t\n",
        "                                   for t in patch_dict[cname][psize][\"results\"]]\n",
        "                           for psize in patch_dict[cname]}\n",
        "                   for cname in patch_dict}\n",
        "    with open(os.path.join(CHECKPOINT_PATH, \"patch_results.json\"), \"w\") as f:\n",
        "        json.dump(result_dict, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g18uLTkWhuEn"
      },
      "source": [
        "Additionally, we implement a function to train and evaluate patches for a list of classes and patch sizes. The pretrained patches include the classes *toaster*, *goldfish*, *school bus*, *lipstick*, and *pineapple*. We chose the classes arbitrarily to cover multiple domains (animals, vehicles, fruits, devices, etc.). We trained each class for three different patch sizes: $32\\times32$ pixels, $48\\times48$ pixels, and $64\\times64$ pixels. We can load them in the two cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APbRyBLYhuEn"
      },
      "outputs": [],
      "source": [
        "def get_patches(class_names, patch_sizes):\n",
        "    result_dict = dict()\n",
        "\n",
        "    # Loop over all classes and patch sizes\n",
        "    for name in class_names:\n",
        "        result_dict[name] = dict()\n",
        "        for patch_size in patch_sizes:\n",
        "            c = label_names.index(name)\n",
        "            file_name = os.path.join(CHECKPOINT_PATH, f\"{name}_{patch_size}_patch.pt\")\n",
        "            # Load patch if pretrained file exists, otherwise start training\n",
        "            if not os.path.isfile(file_name):\n",
        "                patch, val_results = patch_attack(pretrained_model, target_class=c, patch_size=patch_size, num_epochs=5)\n",
        "                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n",
        "                torch.save(patch, file_name)\n",
        "            else:\n",
        "                patch = torch.load(file_name)\n",
        "            # Load evaluation results if exist, otherwise manually evaluate the patch\n",
        "            if name in json_results:\n",
        "                results = json_results[name][str(patch_size)]\n",
        "            else:\n",
        "                results = eval_patch(pretrained_model, patch, data_loader, target_class=c)\n",
        "\n",
        "            # Store results and the patches in a dict for better access\n",
        "            result_dict[name][patch_size] = {\n",
        "                \"results\": results,\n",
        "                \"patch\": patch\n",
        "            }\n",
        "\n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOnOPinShuEn"
      },
      "source": [
        "Feel free to add any additional classes and/or patch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tY6EMAHhuEo"
      },
      "outputs": [],
      "source": [
        "class_names = ['toaster', 'goldfish', 'school bus', 'lipstick', 'pineapple']\n",
        "patch_sizes = [32, 48, 64]\n",
        "\n",
        "patch_dict = get_patches(class_names, patch_sizes)\n",
        "# save_results(patch_dict) # Uncomment if you add new class names and want to save the new results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waNzET3DhuEq"
      },
      "source": [
        "Before looking at the quantitative results, we can actually visualize the patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CcYk4b9huEq"
      },
      "outputs": [],
      "source": [
        "def show_patches():\n",
        "    fig, ax = plt.subplots(len(patch_sizes), len(class_names), figsize=(len(class_names)*2.2, len(patch_sizes)*2.2))\n",
        "    for c_idx, cname in enumerate(class_names):\n",
        "        for p_idx, psize in enumerate(patch_sizes):\n",
        "            patch = patch_dict[cname][psize][\"patch\"]\n",
        "            patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n",
        "            patch = patch.cpu().permute(1, 2, 0).numpy()\n",
        "            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n",
        "            ax[p_idx][c_idx].imshow(patch)\n",
        "            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n",
        "            ax[p_idx][c_idx].axis('off')\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    plt.show()\n",
        "show_patches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO5UnsuJhuEq"
      },
      "source": [
        "**Question**: Comment on the difference between patches of different classes and sizes. E.g., do you see any clear patterns in the patches, ...\n",
        "\n",
        "**Answer**: ...\n",
        "\n",
        "\n",
        "Let's now look at the quantitative results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXhjZCQIhuEq"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!-- Some HTML code to increase font size in the following table -->\n",
        "<style>\n",
        "th {font-size: 120%;}\n",
        "td {font-size: 120%;}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5-RJXh6huEq"
      },
      "outputs": [],
      "source": [
        "import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_table(top_1=True):\n",
        "    i = 0 if top_1 else 1\n",
        "    table = [[name] + [f\"{(100.0 * patch_dict[name][psize]['results'][i]):4.2f}%\" for psize in patch_sizes]\n",
        "             for name in class_names]\n",
        "    display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Class name\"] + [f\"Patch size {psize}x{psize}\" for psize in patch_sizes])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZxJmWT4huEq"
      },
      "source": [
        "First, we will create a table of top-1 accuracy, meaning that how many images have been classified with the target class as highest prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoY7FKfRhuEq"
      },
      "outputs": [],
      "source": [
        "show_table(top_1=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xhfpwEXhuEr"
      },
      "source": [
        "**Question**: Comment on the results. What's the effect of the patch size? Which class is easier to fool, and which is more difficult?\n",
        "\n",
        "**Answer**: ...\n",
        "\n",
        "\n",
        "Let's also take a look at the top-5 accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt4dH40shuEr"
      },
      "outputs": [],
      "source": [
        "show_table(top_1=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFxBM2v9huEr"
      },
      "source": [
        "**Question**: Did you observe the same pattern as the top-1 accuracy? If not, what's the difference?\n",
        "\n",
        "**Answer**: ...\n",
        "\n",
        "Finally, let's create some example visualizations of the patch attack in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrEuAZcQhuEr"
      },
      "outputs": [],
      "source": [
        "def perform_patch_attack(patch):\n",
        "    patch_batch = exmp_batch.clone()\n",
        "    patch_batch = place_patch(patch_batch, patch)\n",
        "    with torch.no_grad():\n",
        "        patch_preds = pretrained_model(patch_batch.to(device))\n",
        "    for i in range(1,17,5):\n",
        "        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kN6KirhhuEr"
      },
      "outputs": [],
      "source": [
        "perform_patch_attack(patch_dict['goldfish'][32]['patch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVTADzB6huEr"
      },
      "source": [
        "The tiny goldfish patch can change all of the predictions to \"goldfish\" as top class. Note that the patch attacks work especially well if the input image is semantically similar to the target class (e.g. a fish and the target class \"goldfish\" works better than an airplane image with that patch). Nevertheless, we can also let the network predict semantically dis-similar classes by using a larger patch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0W5vQy0huEr"
      },
      "outputs": [],
      "source": [
        "perform_patch_attack(patch_dict['school bus'][64]['patch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av9D3jnDhuEr"
      },
      "source": [
        "Although none of the images have anything to do with an American school bus, the high confidence of often 100% shows how powerful such attacks can be. With a few lines of code and access to the model, we were able to generate patches that we add to any image to make the model predict any class we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjHr7wSVhuEr"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have looked at different forms of adversarial attacks. Deep CNNs can be fooled by only slight modifications to the input. Whether it is a carefully designed noise pattern, unnoticeable for a human, or a small patch, we are able to manipulate the networks' predictions significantly.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAfWnrI5d1W6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}